{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yahooo_crawler\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "from statsmodels.api import OLS, add_constant\n",
    "\n",
    "import yahoo_crawler\n",
    "yc = yahoo_crawler.yahoo_crawler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'serif'\n",
    "plt.rcParams['figure.facecolor'] = '1.'\n",
    "plt.rcParams[\"axes.axisbelow\"] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update local stock list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_stocklist_data():\n",
    "    ''' Functions for update stocklist data\n",
    "    Source: www.nasdaq.com\n",
    "    '''\n",
    "    # create stock_list data folder\n",
    "    folder = os.getcwd() + '\\\\stock_list\\\\'\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    # soure url\n",
    "    url = 'https://www.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=%s&render=download'\n",
    "\n",
    "    # available exhanges\n",
    "    exchange = ['nasdaq', 'nyse', 'amex']\n",
    "\n",
    "    for exchg in exchange:\n",
    "        resp = requests.get(url%exchg)\n",
    "        with open(folder + '%s.xlsx'%exchg, 'wb') as output:\n",
    "            output.write(resp.content)\n",
    "    pass\n",
    "\n",
    "# update stock_list\n",
    "update_stocklist_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read local stock list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data folder\n",
    "folder = os.getcwd() + '\\\\stock_list\\\\'\n",
    "\n",
    "# file names\n",
    "files = os.listdir( folder )\n",
    "\n",
    "stolis_df_list = []\n",
    "for f in files:\n",
    "    df = pd.read_csv( folder + f )\n",
    "    stolis_df_list.append(df)\n",
    "    print(f.upper(),df.shape, '\\n==================================================\\n',\n",
    "          df[['Name']].head() )\n",
    "    print('==================================================\\n')\n",
    "\n",
    "# concatenate companies from three exhanges\n",
    "stolis_df_ = pd.concat(stolis_df_list, axis = 0)\n",
    "\n",
    "# drop out fund\n",
    "stolis_df_ = stolis_df_[stolis_df_['industry'] == stolis_df_['industry']]\n",
    "\n",
    "# drop dupplicated company names\n",
    "stolis_df = stolis_df_.drop_duplicates(['Name']).reset_index(drop = True)\n",
    "\n",
    "print('Total %s companies, unique %s companies.' % (stolis_df_.shape[0], stolis_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch yahoo finance stock data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(symbol, start, end,):\n",
    "    t = time.time()\n",
    "    cookie,crumb = yc.get_yahoo_crumb_cookie()\n",
    "\n",
    "    symbol = symbol# stock id\n",
    "    params = {'period1' : start,\n",
    "              'period2' : end,\n",
    "              'interval': '1d',\n",
    "              'events'  : 'history',\n",
    "              'crumb'   : crumb}\n",
    "    df = yc.GetStockPrice(symbol = symbol,\n",
    "                               params = params,\n",
    "                               request_type = 'post')\n",
    "    \n",
    "    # set date as index\n",
    "    df = df.set_index('Date')\n",
    "    df.sort_index(inplace = True)\n",
    "    \n",
    "    t2 = time.time() - t\n",
    "    print(str(round(t2,4))+' seconds elapsed...')\n",
    "    return df[['Adj Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define trading dates with S&P index\n",
    "SP500          = get_stock_data(symbol = '^GSPC', start = '2014-12-01', end = '2019-04-19')\n",
    "standard_index = SP500.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex_data(symbol, tweets_date, standard_index = standard_index, how = 'test'):\n",
    "    \n",
    "    assert how in ['test', 'train'], 'Parameter *how* must be either \\'test\\' or \\'train\\'!'\n",
    "    \n",
    "    if how == 'test':\n",
    "        start = standard_index[ standard_index <  tweets_date][-11]\n",
    "        end   = standard_index[ standard_index >= tweets_date][ 11]\n",
    "\n",
    "        df = get_stock_data(symbol = symbol, start = start, end = end)\n",
    "\n",
    "        df = df.reindex(standard_index)\n",
    "\n",
    "        df_ = pd.concat([ df[ df.index <  tweets_date ].tail(10),\n",
    "                          df[ df.index >= tweets_date ].head(11) ])\n",
    "    \n",
    "    elif how == 'train':\n",
    "        start = standard_index[ standard_index <  tweets_date][-252]\n",
    "        end   = standard_index[ standard_index >= tweets_date][  11]\n",
    "\n",
    "        df = get_stock_data(symbol = symbol, start = start, end = end)\n",
    "\n",
    "        df = df.reindex(standard_index)\n",
    "\n",
    "        df_ = pd.concat([ df[ df.index <  tweets_date ].tail(251),\n",
    "                          df[ df.index >= tweets_date ].head(11) ])\n",
    "        \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch marked tweets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_excel('mentioned_stock.xlsx')\n",
    "tweets['ticker'] = tweets['ticker'].apply(lambda x: x.replace(' ', ''))\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['date'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_i = tweets.iloc[2,:]\n",
    "tweets_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol = tweets_i['ticker']\n",
    "symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_normalize(df):\n",
    "    df = df.copy()\n",
    "    df -= df.mean()\n",
    "    df /= df.std()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_set(symbol, tweets_date, plot = False):\n",
    "    ''' Prepare dataset\n",
    "    '''\n",
    "    \n",
    "    dict_ = {}\n",
    "    ###################################################################################################\n",
    "    #1, Prepare stock price series\n",
    "    S_i = reindex_data(symbol = symbol, tweets_date = tweets_date, how = 'train')\n",
    "\n",
    "    S_m = SP500.reindex(S_i.index)\n",
    "\n",
    "    df = pd.concat([S_i, S_m ], axis = 1)\n",
    "    df.columns = ['S_i', 'S_m']\n",
    "    \n",
    "    assert len(df[df['S_i'] != df['S_i']]) < 10, 'Missing values of stock %s are more than 10!'%symbol\n",
    "    \n",
    "    # fillna for stocks\n",
    "    df = df.fillna(method = 'pad').fillna(method = 'bfill')\n",
    "    \n",
    "    assert len(df[df['S_i'] != df['S_i']]) == 0, 'Failed to fillna for tock %s!'%symbol\n",
    "    \n",
    "    dict_['price'] = df.copy()\n",
    "    ###################################################################################################\n",
    "    #2, Prepare stockprice series\n",
    "    \n",
    "    # calculate log return of stock_i and S&P500\n",
    "    ret = np.log(df).diff().dropna()\n",
    "    \n",
    "    dict_['return'] = ret.copy()\n",
    "    \n",
    "    # regression R_i against R_m\n",
    "    X = add_constant( ret['S_m'].iloc[: 240] )\n",
    "    model = OLS( ret['S_i'].iloc[: 240], X ).fit()\n",
    "    \n",
    "    \n",
    "    # predict S_i\n",
    "    X = add_constant( ret['S_m'] )\n",
    "    R_i_pred = model.predict(X)\n",
    "    \n",
    "    AR = ret['S_i'] - R_i_pred\n",
    "    \n",
    "    # store price, return, parameters, t values, and p values, AR\n",
    "    dict_.update( {\n",
    "        'price'   : df.copy(),\n",
    "        'return'  : ret.copy(),\n",
    "        'params'  : model.params ,\n",
    "        't_values': model.tvalues, \n",
    "        'p_values': model.pvalues,\n",
    "        'AR'      : AR.copy()\n",
    "                 } )\n",
    "    \n",
    "    if plot:\n",
    "        fig = plt.figure(figsize = [10, 6])\n",
    "        \n",
    "        ax1 = fig.add_subplot(221)\n",
    "        ax1.plot(my_normalize(df['S_i']), label = symbol)\n",
    "        ax1.plot(my_normalize(df['S_m']), label = 'S&P500' )\n",
    "        ax1.set_ylabel('Normalize price serires')\n",
    "        ax1.spines['top'  ].set_color('none')\n",
    "        ax1.spines['left' ].set_color('none')\n",
    "        ax1.spines['right'].set_color('none')\n",
    "        ax1.legend(loc = 'best')\n",
    "        ax1.grid(linestyle = ':', axis = 'y')\n",
    "        plt.xticks(rotation = 15)\n",
    "        \n",
    "        ax2 = fig.add_subplot(222)\n",
    "        ax2.plot(ret['S_i'], '-*', label = '%s log return'%symbol)\n",
    "        ax2.plot(ret['S_m'], '-*', label = 'S&P500 log return' )\n",
    "        ax2.set_ylabel('Log Return')\n",
    "        ax2.spines['top'  ].set_color('none')\n",
    "        ax2.spines['left' ].set_color('none')\n",
    "        ax2.spines['right'].set_color('none')\n",
    "        ax2.legend(loc = 'best')\n",
    "        ax2.grid(linestyle = ':', axis = 'y')\n",
    "        ax2.yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "        plt.xticks(rotation = 15)\n",
    "        \n",
    "        ax3 = fig.add_subplot(212)\n",
    "        ax3.plot(AR, '-*', label = '%s AR'%symbol)\n",
    "        ax3.set_ylabel('AR')\n",
    "        ax3.spines['top'  ].set_color('none')\n",
    "        ax3.spines['left' ].set_color('none')\n",
    "        ax3.spines['right'].set_color('none')\n",
    "        ax3.legend(loc = 'best')\n",
    "        ax3.grid(linestyle = ':', axis = 'y')\n",
    "        ax3.yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "        plt.xticks(rotation = 15)\n",
    "        \n",
    "        plt.suptitle('Price, Return, and AR of %s vs. S&P500'%symbol, fontsize = 20)\n",
    "        plt.show()\n",
    "        \n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = fetch_data_set(symbol, tweets_i['date'], plot = True)\n",
    "dict_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mega_dict = {}\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "for i in range( tweets.shape[0] ):\n",
    "    tweets_i    = tweets.iloc[i, :]\n",
    "    symbol      = tweets_i['ticker'].replace(' ', '')\n",
    "    tweets_date = tweets_i['date']\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        mega_dict[ '%s %s'%(symbol,tweets_date) ] = fetch_data_set(symbol, tweets_date, plot = False)\n",
    "        print('Successfully fetched data for %s...'%symbol)\n",
    "        print('==========================================\\n')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Failed to fetch data for %s!'%symbol)\n",
    "        print(e)\n",
    "        print('==========================================\\n')\n",
    "    \n",
    "        \n",
    "print(time.time() - t,'sec(s) elapsed...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mega_dict['F 2019-03-20 20:51:41'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarize OLS result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sig(p_value):\n",
    "    ''' Lable significance level accorning p_value\n",
    "    '''\n",
    "    if p_value > 0.1:\n",
    "        return ''\n",
    "    elif p_value <= 0.1 and p_value > 0.05:\n",
    "        return '*'\n",
    "    elif p_value <= 0.05 and p_value > 0.01:\n",
    "        return '**'\n",
    "    elif p_value <= 0.01:\n",
    "        return '***'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_OLS(mega_dict):\n",
    "    ''' Summarize OLS result\n",
    "    '''\n",
    "    summary = pd.DataFrame()\n",
    "    for k, v in zip( mega_dict.keys(), mega_dict.values() ):\n",
    "        symbol = re.findall(r'(\\w+)\\s2', k)[0]\n",
    "        date   = pd.to_datetime(re.findall(r'%s (.*)'%symbol, k)[0])\n",
    "        \n",
    "        company = tweets[ (tweets['ticker'] == symbol) & (tweets['date'] == date) ]['company'].values[0]\n",
    "        \n",
    "        intercept = '%.5f(%.2f)%s'%( \n",
    "                                v['params'  ]['const'],\n",
    "                                v['t_values']['const'],\n",
    "                                label_sig(v['p_values']['const'])\n",
    "                                    )\n",
    "        \n",
    "        slope = '%.2f(%.2f)%s'%( \n",
    "                                v['params'  ]['S_m'],\n",
    "                                v['t_values']['S_m'],\n",
    "                                label_sig(v['p_values']['S_m'])\n",
    "                                    )\n",
    "        \n",
    "        summary = summary.append( { \n",
    "                                'Stock'    : company,\n",
    "                                'Intercept': intercept, \n",
    "                                'Slope'    : slope\n",
    "                                    } ,ignore_index=True)\n",
    "    return summary[['Stock', 'Intercept', 'Slope']]\n",
    "\n",
    "regression_res = summary_OLS(mega_dict)\n",
    "regression_res.to_csv('coefficients_from_market_model_regression.csv')\n",
    "regression_res.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AAR test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_same_emotion(mega_dict = mega_dict, emotion = 1.):\n",
    "    ''' Collect data set with the same emotion\n",
    "    '''\n",
    "    data_lst = []\n",
    "    for k, v in zip( mega_dict.keys(), mega_dict.values() ):\n",
    "        symbol = re.findall(r'(\\w+)\\s2', k)[0]\n",
    "        date   = pd.to_datetime(re.findall(r'%s (.*)'%symbol, k)[0])\n",
    "        \n",
    "        emotion_ = tweets[ (tweets['ticker'] == symbol ) \\\n",
    "                         & (tweets['date'  ] == date   ) ]['type'].values[0]\n",
    "        \n",
    "        if emotion_ == emotion:\n",
    "            \n",
    "            data_lst.append( v['AR'].reset_index(drop = True) )\n",
    "    return data_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pos = collect_same_emotion(emotion =  1)\n",
    "\n",
    "data_neg = collect_same_emotion(emotion = -1)\n",
    "\n",
    "print('Positive sample size is %s, Negative sample size is %s.'%( len(data_pos), len(data_neg) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_AAR(emotion = 1):\n",
    "    ''' Summarize AAR for specific emotion\n",
    "    '''\n",
    "    data = collect_same_emotion(emotion = emotion)\n",
    "    \n",
    "    size = len( data )\n",
    "    \n",
    "    df = pd.concat( data, axis = 1 )\n",
    "    \n",
    "    sigma = df.head(240).mean(axis = 1).std()\n",
    "    \n",
    "    sigma = df.head(240).mean(axis = 1).std()\n",
    "    \n",
    "    AAR = df.tail(21).mean(axis = 1)\n",
    "    \n",
    "    AAR.index = np.arange(-10, 11, 1)\n",
    "    \n",
    "    t_stats = AAR / sigma\n",
    "    \n",
    "    p_values = t_stats.apply(lambda x:  2 * (  0.5 - abs( t.cdf(x, df = 240 - 2) - 0.5 ) ) ) \n",
    "    \n",
    "    report = pd.concat([AAR, t_stats, p_values], axis = 1)\n",
    "    \n",
    "    report.columns = ['AAR', 't_statistic', 'p_values']\n",
    "    \n",
    "    report['AAR'        ] = report.apply(lambda x: \"{:.2%}\".format(x['AAR']) + label_sig(x['p_values']), axis = 1)\n",
    "    report['t_statistic'] = report['t_statistic'].apply(lambda x: round(x, 4))\n",
    "    report['p_values'   ] = report['p_values'   ].apply(lambda x: round(x, 4))\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_AAR_summary = summary_AAR(emotion =  1)\n",
    "pos_AAR_summary.to_csv('aar_positive_tweets.csv')\n",
    "\n",
    "neg_AAR_summary = summary_AAR(emotion = -1)\n",
    "neg_AAR_summary.to_csv('aar_negative_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

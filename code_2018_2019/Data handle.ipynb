{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update local stock list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_stocklist_data():\n",
    "    ''' Functions for update stocklist data\n",
    "    Source: www.nasdaq.com\n",
    "    '''\n",
    "    # create stock_list data folder\n",
    "    folder = os.getcwd() + '\\\\stock_list\\\\'\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    # soure url\n",
    "    url = 'https://www.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=%s&render=download'\n",
    "\n",
    "    # available exhanges\n",
    "    exchange = ['nasdaq', 'nyse', 'amex']\n",
    "\n",
    "    for exchg in exchange:\n",
    "        resp = requests.get(url%exchg)\n",
    "        with open(folder + '%s.xlsx'%exchg, 'wb') as output:\n",
    "            output.write(resp.content)\n",
    "    pass\n",
    "\n",
    "# update stock_list\n",
    "update_stocklist_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read local stock list and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amex.xlsx', 'nasdaq.xlsx', 'nyse.xlsx']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data folder\n",
    "folder = os.getcwd() + '\\\\stock_list\\\\'\n",
    "\n",
    "# file names\n",
    "files = os.listdir( folder )\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMEX.XLSX (309, 9) \n",
      "==================================================\n",
      "                                                 Name\n",
      "0                            22nd Century Group, Inc\n",
      "1              Aberdeen Asia-Pacific Income Fund Inc\n",
      "2                 Aberdeen Australia Equity Fund Inc\n",
      "3  Aberdeen Emerging Markets Equity Income Fund, ...\n",
      "4                  Aberdeen Global Income Fund, Inc.\n",
      "==================================================\n",
      "\n",
      "NASDAQ.XLSX (3450, 9) \n",
      "==================================================\n",
      "                                      Name\n",
      "0                               111, Inc.\n",
      "1  1347 Property Insurance Holdings, Inc.\n",
      "2  1347 Property Insurance Holdings, Inc.\n",
      "3                180 Degree Capital Corp.\n",
      "4                 1-800 FLOWERS.COM, Inc.\n",
      "==================================================\n",
      "\n",
      "NYSE.XLSX (3104, 9) \n",
      "==================================================\n",
      "                      Name\n",
      "0  3D Systems Corporation\n",
      "1              3M Company\n",
      "2         500.com Limited\n",
      "3             58.com Inc.\n",
      "4                 8x8 Inc\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stolis_df_list = []\n",
    "for f in files:\n",
    "    df = pd.read_csv( folder + f )\n",
    "    stolis_df_list.append(df)\n",
    "    print(f.upper(),df.shape, '\\n==================================================\\n',\n",
    "          df[['Name']].head() )\n",
    "    print('==================================================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 5307 companies, unique 4796 companies.\n"
     ]
    }
   ],
   "source": [
    "# concatenate companies from three exhanges\n",
    "stolis_df_ = pd.concat(stolis_df_list, axis = 0)\n",
    "\n",
    "# drop out fund\n",
    "stolis_df_ = stolis_df_[stolis_df_['industry'] == stolis_df_['industry']]\n",
    "\n",
    "# drop dupplicated company names\n",
    "stolis_df = stolis_df_.drop_duplicates(['Name']).reset_index(drop = True)\n",
    "\n",
    "print('Total %s companies, unique %s companies.' % (stolis_df_.shape[0], stolis_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare tweets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'Data handle.ipynb',\n",
       " 'data.csv',\n",
       " 'data.xlsx',\n",
       " 'event_selecting_logic.md',\n",
       " 'marked_tweets.xlsx',\n",
       " 'nasdaq.xls',\n",
       " 'stock_list',\n",
       " 'tweets.txt',\n",
       " 'tweets_data.csv',\n",
       " 'tweets_data.xlsx',\n",
       " 'word frequency.csv',\n",
       " 'words_database.xlsx',\n",
       " 'words_databse(1st_cleaned).xlsx',\n",
       " 'word_list.csv',\n",
       " 'word_lst.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename\n",
    "filename = 'tweets.txt'\n",
    "\n",
    "# read txt file\n",
    "file = open(filename).read()\n",
    "\n",
    "# convert json format to dataframe\n",
    "data = pd.DataFrame(eval(file.replace('false', 'False').replace('true', 'True')))\n",
    "\n",
    "# store tweets in excel\n",
    "data.to_excel('data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in(string, str_lst = ['data'], lower = True):\n",
    "    ''' Detect whether words in *str_lst* exist in *string* or not.\n",
    "    Input:\n",
    "    \n",
    "    -- string: string for examing\n",
    "               str format\n",
    "               \n",
    "    -- str_lst: a list of key words\n",
    "                list of str\n",
    "                default is *[' data ']*\n",
    "    \n",
    "    -- lower: determine whether capital letter is ignored or not, \n",
    "              True -> ignore capital letters, transform all string to lower case;\n",
    "              Fasle -> capital letters can't be ignored, both in *string* and *str_lst*.\n",
    "    \n",
    "    Output:\n",
    "    \n",
    "    -- if any key words is detedted:\n",
    "           return a string with all key words emphathized\n",
    "           \n",
    "       else:\n",
    "           return numpy.nan\n",
    "    '''\n",
    "    string = ' %s '%string\n",
    "    if lower:\n",
    "        \n",
    "        def my_lable(string, str_lst = str_lst):\n",
    "            for s in str_lst:\n",
    "                string = re.sub(r'(?<=\\W)%s(?=\\W)'%s.lower(), ' {%s} '%s.upper(), string )\n",
    "            return string\n",
    "\n",
    "        string = string.lower()\n",
    "\n",
    "        if my_lable(string, str_lst = str_lst) != string:\n",
    "            return my_lable(string)\n",
    "\n",
    "        else:\n",
    "            return np.nan\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        def my_lable(string, str_lst = str_lst):\n",
    "            for s in str_lst:\n",
    "                string = re.sub(r'(?<=\\W)%s(?=\\W)'%s, ' {%s} '%s.upper(), string )\n",
    "            return string\n",
    "\n",
    "        string = string\n",
    "\n",
    "        if my_lable(string, str_lst = str_lst) != string:\n",
    "            return my_lable(string)\n",
    "\n",
    "        else:\n",
    "            return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pattern(string, dict_):\n",
    "    pattern = re.compile(r'{ (.*?) }', re.S)\n",
    "    items = re.findall(pattern, string)\n",
    "    for i in items:\n",
    "        try:\n",
    "            dict_[i] += 1.\n",
    "        except:\n",
    "            dict_[i] = 1.\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['created_at', 'favorite_count', 'id_str', 'is_retweet', 'retweet_count',\n",
       "       'source', 'text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['created_at'] = pd.to_datetime( data['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: re.sub('https://\\S+', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search STR_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def search_str_lst(str_lst, print_ = True):\n",
    "    assert type(str_lst) == list, 'Type of str_lst must be list!'\n",
    "    i = 1\n",
    "    dict_ = {}\n",
    "    v_lst = data['text'].apply(is_in, str_lst= str_lst).dropna().values\n",
    "    if print_:\n",
    "        for v in v_lst:\n",
    "            dict_ = count_pattern(v, dict_)\n",
    "            print(v)\n",
    "            print('================================================%s/%s\\n'%( i, len(v_lst) ))\n",
    "            i += 1\n",
    "    return v_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i  {AGREE}  with kim jong un of north korea that our personal relationship remains very good, perhaps the term excellent would be even more accurate, and that a third summit would be good in that we fully understand where we each stand. north korea has tremendous potential for....... \n",
      "================================================1/30\n",
      "\n",
      " more apprehensions (captures)\n",
      "at the southern border than in many years. border patrol amazing! country is full! system has been broken for many years. democrats in congress must  {AGREE}  to fix loopholes - no open borders (crimes &amp; drugs). will close southern border if necessary... \n",
      "================================================2/30\n",
      "\n",
      " “the lowest average jobs number for any president since 1951, 4.1%. economy doing great. if the democrats win, it is all over.” @varneyco  @foxandfriends  i  {AGREE} ! \n",
      "================================================3/30\n",
      "\n",
      " wow! a suffolk/usa today poll, just out, states, “50% of americans  {AGREE}  that  robert mueller’s investigation is a witch hunt.” @msnbc  very few think it is legit! we will soon find out? \n",
      "================================================4/30\n",
      "\n",
      " prominent legal scholars  {AGREE}  that our actions to address the national emergency at the southern border and to protect the american people are both constitutional and expressly authorized by congress.... \n",
      "================================================5/30\n",
      "\n",
      " i  {AGREE}  with rand paul. this is a total disgrace and should never happen to another president!  \n",
      "================================================6/30\n",
      "\n",
      " rt @gopchairwoman: i completely  {AGREE}  with kevin mccarthy.… \n",
      "================================================7/30\n",
      "\n",
      " ....this will be remembered as one of the most shocking votes in the history of congress. if there is one thing we should all  {AGREE}  on, it’s protecting the lives of innocent babies. \n",
      "================================================8/30\n",
      "\n",
      " rt @club4growth: we  {AGREE} ! the time is now! ?? \"the time has come to pass school choice for america's children.\" - @realdonaldtrump #sotu \n",
      "================================================9/30\n",
      "\n",
      " ....meeting with their top leaders and representatives today in the oval office. no final deal will be made until my friend president xi, and i, meet in the near future to discuss and  {AGREE}  on some of the long standing and more difficult points. very comprehensive transaction.... \n",
      "================================================10/30\n",
      "\n",
      " “in the media’s effort to destroy the president, they are actually destroying themselves. given all of the tremendous headwinds this president has faced, it’s amazing he has accomplished so much.” deroy murdock @foxandfriends  i  {AGREE} ! \n",
      "================================================11/30\n",
      "\n",
      " howard schultz doesn’t have the “guts” to run for president! watched him on @60minutes last night and i  {AGREE}  with him that he is not the “smartest person.” besides, america already has that! i only hope that starbucks is still paying me their rent in trump tower! \n",
      "================================================12/30\n",
      "\n",
      " president and mrs. obama built/has a ten foot wall around their d.c. mansion/compound. i  {AGREE} , totally necessary for their safety and security. the u.s. needs the same thing, slightly larger version! \n",
      "================================================13/30\n",
      "\n",
      " i totally  {AGREE} !  \n",
      "================================================14/30\n",
      "\n",
      " thanks @randpaul  “i am very proud of the president. this is exactly what he promised, and i think the people  {AGREE}  with him. we’ve been at war too long and in too many places...spent several trillion dollars on these wars everywhere. he’s different...that’s why he got elected.” \n",
      "================================================15/30\n",
      "\n",
      " “i’m proud of the president today to hear that he is declaring victory in syria.” senator rand paul.  “i couldn’t  {AGREE}  more with the presidents decision. by definition, this is the opposite of an obama decision. senator mike lee \n",
      "================================================16/30\n",
      "\n",
      " statement from china: “the teams of both sides are now having smooth communications and good cooperation with each other. we are full of confidence that an agreement can be reached within the next 90 days.” i  {AGREE} ! \n",
      "================================================17/30\n",
      "\n",
      " “it’s a mean &amp; nasty world out there, the middle east in particular. this is a long and historic commitment, &amp; one that is absolutely vital to america’s national security.” @secpompeo  i  {AGREE}  100%. in addition, many billions of dollars of purchases made in u.s., big jobs &amp; oil! \n",
      "================================================18/30\n",
      "\n",
      " .@davidasmanfox  “how do the democrats respond to this? think of how his position with republicans improves-all the candidates who won tonight. they realize how important he is because of what he did in campaigning for them. they owe him their political career.” thanks, i  {AGREE} ! \n",
      "================================================19/30\n",
      "\n",
      " so-called birthright citizenship, which costs our country billions of dollars and is very unfair to our citizens, will be ended one way or the other. it is not covered by the 14th amendment because of the words “subject to the jurisdiction thereof.” many legal scholars  {AGREE} ..... \n",
      "================================================20/30\n",
      "\n",
      " ...i  {AGREE}  with their stance 100%, and the united states is likewise taking a very hard line on illegal immigration. the prime minister is working very hard on the economy of italy - he will be successful! \n",
      "================================================21/30\n",
      "\n",
      " i  {AGREE}  wholeheartedly!  \n",
      "================================================22/30\n",
      "\n",
      " i  {AGREE}  with president obama 100%!  \n",
      "================================================23/30\n",
      "\n",
      " if the democrats would stop being obstructionists and come together, we could write up and  {AGREE}  to new immigration laws in less than one hour. look at the needless  pain and suffering that they are causing. look at the horrors taking place on the border. chuck &amp; nancy, call me! \n",
      "================================================24/30\n",
      "\n",
      " chuck schumer, i  {AGREE} !  \n",
      "================================================25/30\n",
      "\n",
      " the economy is doing perhaps better than ever before, and that’s prior to fixing some of the worst and most unfair trade deals ever made by any country. in any event, they are coming along very well. most countries  {AGREE}  that they must be changed, but nobody ever asked! \n",
      "================================================26/30\n",
      "\n",
      " democrats mistakenly tweet 2014 pictures from obama’s term showing children from the border in steel cages. they thought it was recent pictures in order to make us look bad, but backfires. dems must  {AGREE}  to wall and new border protection for good of country...bipartisan bill! \n",
      "================================================27/30\n",
      "\n",
      " department of justice should have urged the supreme court to at least hear the drivers license case on illegal immigrants in arizona. i  {AGREE}  with @loudobbs. should have sought review. \n",
      "================================================28/30\n",
      "\n",
      " “he’s got a very good point. somebody in the justice department has a treasure trove of evidence of mrs. clinton’s criminality at her own hands, or through others, that ought to be investigated. i fully  {AGREE}  with the president on that.” @judgenapolitano on @marthamaccallum show \n",
      "================================================29/30\n",
      "\n",
      " thank you @gopleader kevin mccarthy! couldn’t  {AGREE}  w/you more. together, we are #maga????  \n",
      "================================================30/30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "v_lst = search_str_lst(['AGREE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drop_lst = '''Company,Laboratories,Inc,plc,corp,group,equities,pharmaceutical,technology,plc,coporation,co,Resource,\n",
    "networks,green,texas,holdings,Inc,properties,holdings,energy,communications,limited,solutions,resources,brands,SUMMIT,\n",
    "hunt,companies,health,restrants,services,chemical,int,l,arts,resources,Holdings,Holding,Inc,Cos,Ltd,Corp,Co,plc,PEOPLE,\n",
    "red,space,under,Cos,Group,properties,Corporation,Incorporated,tree,business,city,Residential,Company,TOTAL,one,aid,up,\n",
    "line,gas,network,black,federal,union ,best,air,water,U,S,Trust,Arts,Communications,Chemical,Lifesciences,JUST,usa,\n",
    "Technologies,Systems,General,First,Street,Southern,Networks,Realty,Service,Class,A,Materials,Class,Cruise,Line,180,\n",
    "Services,Financial,Resources,NATIONAL ,Foods,Scientific,Beauty,Realty,Communications,com,Automotive,Stores,Mueller,\n",
    "Technologies,International,WEST ,Markets,Machines,Sciences,Exchange,Tool,Works,Dynamics,Bank,Investment,limited,Simply,\n",
    "Laboratories,NEWS ,technology,Resources,Resorts,equities,energy,health,Parts,brands,and,a,at,on,take,of,Church,forward,\n",
    "system,new,UNITED,Republic,OIL ,real,york,the,AMERICAN, america,state,C,Data,SECURITY ,companies,restrants,PLANS,can,\n",
    "Industries,Gold,Management,Education,REIT,Acquisition,Partners,LP,China,Hospitality,Medical,Capital,Royalty,world,\n",
    "Electronics,39,Enterprisesde,Investors,Industrial,Power,Products,Property,Insurance,Finance,Life,Worldwide,Electric,\n",
    "if,now,all,care,nation,by,do,mexico,it,in,me,CHECK,great,sports,good,golf,big,AGREE,GROWTH,north,world,korea,forward,\n",
    "Software,Pacific,Global,Bio,Entertainment,Media,Community,Estate,LLC,Hotels,for,Cool,first,second,third,fifth,fourth,\n",
    "Strong,Healthcare,Cohen,Standard,Star,Opportunity,Level,Plus,HealthCare,Georgia,Place,States,Information,800,Wisconsin,\n",
    "Source,Mark,Public,Way,Manufacturing,Funding,Better,South,Carolina,James,Beyond,Stock,Private,Career,European,Point,\n",
    "Children,Citizens,Clean,Center,Consumer,County,Old,Country,Credit,Journal,Dollar,Victory,TRADE,Montana,East,Focus,\n",
    "Ever,Live,Payments,Washington,Stay,Farmers,Choice,Indiana,Foundation,US,Five,Prime,Full,House,Fusion,Future,Times,Fix,\n",
    "Troy,Henry,Home,Hope,Building,Infrastructure,Support,Money,Japan,Control,John,Kelly,Defense,End,Smart,Marine,Merit,\n",
    "Con,Modern,Mr,My,NICE,Office,Ohio,Steel,Stop,Open,Re,Patrick,Virginia,Points,Popular,Positive,Progress,Safe,Safety,\n",
    "Special,Games,Florida,Number,SMART,Missouri,Spirit,Market,support,Price,Two,Long,Joint,Meet,Trade,Top,TOP,Tuesday,de,\n",
    "Wins,500,Interest,Communities,Pittsburgh,Lots,Cia,Family,Build,Canada,Cars,Fair,Israel,Clear,well,40,,Fire,Champion,\n",
    "Dr,Government,Mississippi,Far,Fortune,Host,Game,Las,Argentina,Europe,Party,Pennsylvania,Post,RE,Ready,Robert,San,Six,\n",
    "Joe,Team,France,Tennessee,AS,Met,Waters,White,Therapeutics,CALIFORNIA,strong,Recovery,Pharmaceuticals,Morning,Harvard\n",
    "'''.replace(' ', '').replace('\\n', '').lower().split(',')\n",
    "\n",
    "df = pd.read_excel('words_databse(1st_cleaned).xlsx')\n",
    "drop_lst += df[df['drop'] == 0].index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_set = {*drop_lst}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Company name search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22nd Century Group, Inc</td>\n",
       "      <td>[22nd, Century, Group, Inc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acme United Corporation.</td>\n",
       "      <td>[Acme, United, Corporation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Actinium Pharmaceuticals, Inc.</td>\n",
       "      <td>[Actinium, Pharmaceuticals, Inc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adams Resources &amp; Energy, Inc.</td>\n",
       "      <td>[Adams, Resources, Energy, Inc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AeroCentury Corp.</td>\n",
       "      <td>[AeroCentury, Corp]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Name                             words\n",
       "0         22nd Century Group, Inc       [22nd, Century, Group, Inc]\n",
       "1        Acme United Corporation.       [Acme, United, Corporation]\n",
       "2  Actinium Pharmaceuticals, Inc.  [Actinium, Pharmaceuticals, Inc]\n",
       "3  Adams Resources & Energy, Inc.   [Adams, Resources, Energy, Inc]\n",
       "4               AeroCentury Corp.               [AeroCentury, Corp]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split stock names\n",
    "name_df = stolis_df[['Name']].copy()\n",
    "\n",
    "name_df['words'] = pd.DataFrame( name_df['Name'].apply(lambda x: \\\n",
    "                                                        re.findall(r'(\\w+)', x) ) )\n",
    "name_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knock_out(lst, drop_lst = drop_set):\n",
    "    ''' Knock out words in drop_set\n",
    "    '''\n",
    "    lst_ = []\n",
    "    for i in lst:\n",
    "        # if in drop_lst or length < 2, drop it\n",
    "        if i.lower() in drop_lst or len(i)< 2:\n",
    "            pass\n",
    "        else:\n",
    "            lst_.append(i)\n",
    "            \n",
    "    if len(lst_) > 0:\n",
    "        return lst_\n",
    "    \n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "name_df['dropped'] = name_df['words'].apply(knock_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214 companies are dropped out from stock list.\n"
     ]
    }
   ],
   "source": [
    "dropped_companies = name_df[name_df['dropped'] != name_df['dropped']]\n",
    "print('%s companies are dropped out from stock list.'%dropped_companies.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4582 companies are kept after dropping out.\n"
     ]
    }
   ],
   "source": [
    "name_df = name_df.dropna()\n",
    "print('%s companies are kept after dropping out.'%name_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-arrange key-words and companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>words</th>\n",
       "      <th>dropped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22nd Century Group, Inc</td>\n",
       "      <td>[22nd, Century, Group, Inc]</td>\n",
       "      <td>[Century]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acme United Corporation.</td>\n",
       "      <td>[Acme, United, Corporation]</td>\n",
       "      <td>[Acme]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Actinium Pharmaceuticals, Inc.</td>\n",
       "      <td>[Actinium, Pharmaceuticals, Inc]</td>\n",
       "      <td>[Actinium]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adams Resources &amp; Energy, Inc.</td>\n",
       "      <td>[Adams, Resources, Energy, Inc]</td>\n",
       "      <td>[Adams]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AeroCentury Corp.</td>\n",
       "      <td>[AeroCentury, Corp]</td>\n",
       "      <td>[AeroCentury]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Name                             words  \\\n",
       "0         22nd Century Group, Inc       [22nd, Century, Group, Inc]   \n",
       "1        Acme United Corporation.       [Acme, United, Corporation]   \n",
       "2  Actinium Pharmaceuticals, Inc.  [Actinium, Pharmaceuticals, Inc]   \n",
       "3  Adams Resources & Energy, Inc.   [Adams, Resources, Energy, Inc]   \n",
       "4               AeroCentury Corp.               [AeroCentury, Corp]   \n",
       "\n",
       "         dropped  \n",
       "0      [Century]  \n",
       "1         [Acme]  \n",
       "2     [Actinium]  \n",
       "3        [Adams]  \n",
       "4  [AeroCentury]  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_words_dict_data():\n",
    "    words_dict = {}\n",
    "    for i in range( name_df.shape[0] ):\n",
    "        words = name_df['dropped'].values[i]\n",
    "        comp  = name_df['Name'   ].values[i]\n",
    "\n",
    "        for w in words:\n",
    "            try:\n",
    "                words_dict[w] += '%s, '%comp\n",
    "            except:\n",
    "                words_dict[w] = ''\n",
    "                words_dict[w] += '%s, '%comp\n",
    "    pd.DataFrame(words_dict, index = [0]).T.to_excel('words_database.xlsx')\n",
    "    pass\n",
    "save_words_dict_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_dict = {}\n",
    "for i in range( name_df.shape[0] ):\n",
    "    words = name_df['dropped'].values[i]\n",
    "    comp  = name_df['Name'   ].values[i]\n",
    "    \n",
    "    for w in words:\n",
    "        try:\n",
    "            words_dict[w] += ['%s, '%comp]\n",
    "        except:\n",
    "            words_dict[w] = []\n",
    "            words_dict[w] += ['%s, '%comp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop zero search frequency key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0862 sec(s) elapsed, 417.9666 sec(s) left, total 427.0529 sec(s)\n",
      "17.4863 sec(s) elapsed, 393.4419 sec(s) left, total 410.9282 sec(s)\n",
      "26.0383 sec(s) elapsed, 381.8947 sec(s) left, total 407.9330 sec(s)\n",
      "34.1802 sec(s) elapsed, 367.4367 sec(s) left, total 401.6169 sec(s)\n",
      "42.4353 sec(s) elapsed, 356.4565 sec(s) left, total 398.8917 sec(s)\n",
      "51.0381 sec(s) elapsed, 348.7603 sec(s) left, total 399.7984 sec(s)\n",
      "59.2125 sec(s) elapsed, 338.3571 sec(s) left, total 397.5695 sec(s)\n",
      "67.3377 sec(s) elapsed, 328.2712 sec(s) left, total 395.6089 sec(s)\n",
      "75.4449 sec(s) elapsed, 318.5451 sec(s) left, total 393.9900 sec(s)\n",
      "83.5930 sec(s) elapsed, 309.2942 sec(s) left, total 392.8872 sec(s)\n",
      "91.8828 sec(s) elapsed, 300.7072 sec(s) left, total 392.5899 sec(s)\n",
      "100.0477 sec(s) elapsed, 291.8058 sec(s) left, total 391.8535 sec(s)\n",
      "108.6478 sec(s) elapsed, 284.1558 sec(s) left, total 392.8036 sec(s)\n",
      "117.0977 sec(s) elapsed, 276.0160 sec(s) left, total 393.1138 sec(s)\n",
      "125.2644 sec(s) elapsed, 267.2307 sec(s) left, total 392.4951 sec(s)\n",
      "133.4600 sec(s) elapsed, 258.5787 sec(s) left, total 392.0386 sec(s)\n",
      "141.6632 sec(s) elapsed, 249.9939 sec(s) left, total 391.6571 sec(s)\n",
      "150.2808 sec(s) elapsed, 242.1191 sec(s) left, total 392.3999 sec(s)\n",
      "158.5320 sec(s) elapsed, 233.6261 sec(s) left, total 392.1581 sec(s)\n",
      "166.7547 sec(s) elapsed, 225.1189 sec(s) left, total 391.8737 sec(s)\n",
      "174.8639 sec(s) elapsed, 216.4981 sec(s) left, total 391.3620 sec(s)\n",
      "183.0153 sec(s) elapsed, 207.9719 sec(s) left, total 390.9872 sec(s)\n",
      "191.1555 sec(s) elapsed, 199.4666 sec(s) left, total 390.6220 sec(s)\n",
      "199.8851 sec(s) elapsed, 191.5565 sec(s) left, total 391.4416 sec(s)\n",
      "208.0570 sec(s) elapsed, 183.0901 sec(s) left, total 391.1471 sec(s)\n",
      "216.2178 sec(s) elapsed, 174.6374 sec(s) left, total 390.8552 sec(s)\n",
      "224.4405 sec(s) elapsed, 166.2522 sec(s) left, total 390.6927 sec(s)\n",
      "232.7651 sec(s) elapsed, 157.9477 sec(s) left, total 390.7128 sec(s)\n",
      "240.9842 sec(s) elapsed, 149.5764 sec(s) left, total 390.5606 sec(s)\n",
      "249.3684 sec(s) elapsed, 141.3087 sec(s) left, total 390.6771 sec(s)\n",
      "257.7061 sec(s) elapsed, 133.0096 sec(s) left, total 390.7157 sec(s)\n",
      "266.8117 sec(s) elapsed, 125.0680 sec(s) left, total 391.8796 sec(s)\n",
      "274.9605 sec(s) elapsed, 116.6499 sec(s) left, total 391.6104 sec(s)\n",
      "283.1899 sec(s) elapsed, 108.2785 sec(s) left, total 391.4684 sec(s)\n",
      "291.3918 sec(s) elapsed, 99.9058 sec(s) left, total 391.2975 sec(s)\n",
      "299.5931 sec(s) elapsed, 91.5423 sec(s) left, total 391.1355 sec(s)\n",
      "307.7525 sec(s) elapsed, 83.1763 sec(s) left, total 390.9288 sec(s)\n",
      "315.9254 sec(s) elapsed, 74.8244 sec(s) left, total 390.7498 sec(s)\n",
      "324.1191 sec(s) elapsed, 66.4860 sec(s) left, total 390.6051 sec(s)\n",
      "332.2819 sec(s) elapsed, 58.1493 sec(s) left, total 390.4312 sec(s)\n",
      "340.5337 sec(s) elapsed, 49.8342 sec(s) left, total 390.3679 sec(s)\n",
      "349.4217 sec(s) elapsed, 41.5978 sec(s) left, total 391.0195 sec(s)\n",
      "359.2843 sec(s) elapsed, 33.4218 sec(s) left, total 392.7061 sec(s)\n",
      "367.4187 sec(s) elapsed, 25.0513 sec(s) left, total 392.4700 sec(s)\n",
      "375.6719 sec(s) elapsed, 16.6965 sec(s) left, total 392.3684 sec(s)\n",
      "384.2347 sec(s) elapsed, 8.3529 sec(s) left, total 392.5877 sec(s)\n",
      "392.3992 sec(s) elapsed, 0.0000 sec(s) left, total 392.3992 sec(s)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "t = time.time()\n",
    "k_lst = words_dict.copy().keys()\n",
    "for k in k_lst:\n",
    "    if len(search_str_lst([k], False)) == 0:\n",
    "        words_dict.pop(k)\n",
    "        \n",
    "    i += 1\n",
    "    T = time.time() - t\n",
    "    v = T/i\n",
    "    Tt = len(k_lst) * v\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print('%.4f sec(s) elapsed,'%T, '%.4f sec(s) left,'%(Tt - T), 'total %.4f sec(s)'%Tt )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "621"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find mentioned companies and mark key words out in tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "t = time.time()\n",
    "def find_comp(string, words_dict = words_dict.copy(), lower = True,):\n",
    "    ''' Find companies\n",
    "    '''\n",
    "    #====================================================\n",
    "    string = ' %s '%string.lower()\n",
    "        \n",
    "    def my_label(string, str_lst):\n",
    "        for s in str_lst:\n",
    "            string = re.sub(r'(?<=\\W)%s(?=\\W)'%s.lower(), ' {%s} '%s.upper(), string )\n",
    "        return string\n",
    "    \n",
    "    str_lst  = []\n",
    "    comp_lst = []\n",
    "    \n",
    "    for k in words_dict.keys():\n",
    "        \n",
    "        comp = words_dict[k]\n",
    "        \n",
    "        if lower:\n",
    "            k = k.lower()\n",
    "            \n",
    "            if  string != my_label(string, str_lst = [k] ):\n",
    "                str_lst  += [k]\n",
    "                \n",
    "                comp_lst += comp\n",
    "            \n",
    "        else:\n",
    "            if  string != my_label(string, str_lst = [k] ):\n",
    "                str_lst  += [k]\n",
    "                \n",
    "                comp_lst += comp\n",
    "                \n",
    "    global i\n",
    "    i += 1\n",
    "    T = time.time() - t\n",
    "    v = T/i\n",
    "    Tt = data['text'].shape[0] * v\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print('%.4f sec(s) elapsed,'%T, '%.4f sec(s) left,'%(Tt - T), 'total %.4f sec(s)'%Tt )\n",
    "    \n",
    "    if len(str_lst) > 0:\n",
    "        return my_label(string, str_lst = {*str_lst}), ''.join( list( {*comp_lst} ) )\n",
    "    \n",
    "    else:\n",
    "        return np.nan, np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.7416 sec(s) elapsed, 361.2988 sec(s) left, total 369.0403 sec(s)\n",
      "15.7564 sec(s) elapsed, 359.7965 sec(s) left, total 375.5529 sec(s)\n",
      "23.2437 sec(s) elapsed, 346.0986 sec(s) left, total 369.3423 sec(s)\n",
      "30.8240 sec(s) elapsed, 336.5213 sec(s) left, total 367.3453 sec(s)\n",
      "38.3537 sec(s) elapsed, 327.3103 sec(s) left, total 365.6640 sec(s)\n",
      "46.4162 sec(s) elapsed, 322.3604 sec(s) left, total 368.7766 sec(s)\n",
      "54.0439 sec(s) elapsed, 313.9952 sec(s) left, total 368.0391 sec(s)\n",
      "61.5847 sec(s) elapsed, 305.3832 sec(s) left, total 366.9679 sec(s)\n",
      "69.3752 sec(s) elapsed, 298.0821 sec(s) left, total 367.4573 sec(s)\n",
      "77.0724 sec(s) elapsed, 290.3317 sec(s) left, total 367.4041 sec(s)\n",
      "84.8583 sec(s) elapsed, 282.8867 sec(s) left, total 367.7450 sec(s)\n",
      "92.5271 sec(s) elapsed, 275.0367 sec(s) left, total 367.5638 sec(s)\n",
      "100.3056 sec(s) elapsed, 267.5074 sec(s) left, total 367.8131 sec(s)\n",
      "108.2415 sec(s) elapsed, 260.3209 sec(s) left, total 368.5625 sec(s)\n",
      "116.8442 sec(s) elapsed, 254.4867 sec(s) left, total 371.3309 sec(s)\n",
      "124.6091 sec(s) elapsed, 246.6482 sec(s) left, total 371.2573 sec(s)\n",
      "132.3879 sec(s) elapsed, 238.8434 sec(s) left, total 371.2314 sec(s)\n",
      "140.0222 sec(s) elapsed, 230.8032 sec(s) left, total 370.8254 sec(s)\n",
      "147.6419 sec(s) elapsed, 222.7839 sec(s) left, total 370.4259 sec(s)\n",
      "155.3239 sec(s) elapsed, 214.8906 sec(s) left, total 370.2146 sec(s)\n",
      "162.9523 sec(s) elapsed, 206.9494 sec(s) left, total 369.9016 sec(s)\n",
      "170.6296 sec(s) elapsed, 199.0937 sec(s) left, total 369.7234 sec(s)\n",
      "178.1634 sec(s) elapsed, 191.0996 sec(s) left, total 369.2630 sec(s)\n",
      "185.7693 sec(s) elapsed, 183.2150 sec(s) left, total 368.9843 sec(s)\n",
      "193.4923 sec(s) elapsed, 175.4588 sec(s) left, total 368.9511 sec(s)\n",
      "201.0892 sec(s) elapsed, 167.6001 sec(s) left, total 368.6894 sec(s)\n",
      "208.9608 sec(s) elapsed, 159.9711 sec(s) left, total 368.9319 sec(s)\n",
      "218.5864 sec(s) elapsed, 153.5570 sec(s) left, total 372.1434 sec(s)\n",
      "226.4021 sec(s) elapsed, 145.7561 sec(s) left, total 372.1582 sec(s)\n",
      "234.5656 sec(s) elapsed, 138.1591 sec(s) left, total 372.7247 sec(s)\n",
      "242.2579 sec(s) elapsed, 130.2722 sec(s) left, total 372.5301 sec(s)\n",
      "249.9982 sec(s) elapsed, 122.4210 sec(s) left, total 372.4192 sec(s)\n",
      "257.6943 sec(s) elapsed, 114.5568 sec(s) left, total 372.2511 sec(s)\n",
      "265.3996 sec(s) elapsed, 106.7062 sec(s) left, total 372.1058 sec(s)\n",
      "273.0716 sec(s) elapsed, 98.8519 sec(s) left, total 371.9235 sec(s)\n",
      "280.7755 sec(s) elapsed, 91.0181 sec(s) left, total 371.7936 sec(s)\n",
      "288.7913 sec(s) elapsed, 83.2812 sec(s) left, total 372.0725 sec(s)\n",
      "297.5266 sec(s) elapsed, 75.7127 sec(s) left, total 373.2392 sec(s)\n",
      "305.2675 sec(s) elapsed, 67.8633 sec(s) left, total 373.1308 sec(s)\n",
      "313.2536 sec(s) elapsed, 60.0664 sec(s) left, total 373.3200 sec(s)\n",
      "320.9300 sec(s) elapsed, 52.2098 sec(s) left, total 373.1398 sec(s)\n",
      "328.6262 sec(s) elapsed, 44.3645 sec(s) left, total 372.9908 sec(s)\n",
      "336.3178 sec(s) elapsed, 36.5257 sec(s) left, total 372.8435 sec(s)\n",
      "343.9994 sec(s) elapsed, 28.6927 sec(s) left, total 372.6921 sec(s)\n",
      "352.3043 sec(s) elapsed, 20.9034 sec(s) left, total 373.2076 sec(s)\n",
      "360.0534 sec(s) elapsed, 13.0715 sec(s) left, total 373.1249 sec(s)\n",
      "367.7837 sec(s) elapsed, 5.2429 sec(s) left, total 373.0265 sec(s)\n"
     ]
    }
   ],
   "source": [
    "res = data['text'].apply(lambda x: pd.Series( find_comp(x) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.columns = ['tweets', 'companies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([res, data['created_at']], axis = 1).dropna().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_excel('marked_tweets.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

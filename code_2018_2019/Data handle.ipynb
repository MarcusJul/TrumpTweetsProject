{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update local stock list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_stocklist_data():\n",
    "    ''' Functions for update stocklist data\n",
    "    Source: www.nasdaq.com\n",
    "    '''\n",
    "    # create stock_list data folder\n",
    "    folder = os.getcwd() + '\\\\stock_list\\\\'\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    # soure url\n",
    "    url = 'https://www.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=%s&render=download'\n",
    "\n",
    "    # available exhanges\n",
    "    exchange = ['nasdaq', 'nyse', 'amex']\n",
    "\n",
    "    for exchg in exchange:\n",
    "        resp = requests.get(url%exchg)\n",
    "        with open(folder + '%s.xlsx'%exchg, 'wb') as output:\n",
    "            output.write(resp.content)\n",
    "    pass\n",
    "\n",
    "# update stock_list\n",
    "update_stocklist_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read local stock list and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amex.xlsx', 'nasdaq.xlsx', 'nyse.xlsx']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data folder\n",
    "folder = os.getcwd() + '\\\\stock_list\\\\'\n",
    "\n",
    "# file names\n",
    "files = os.listdir( folder )\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMEX.XLSX (309, 9) \n",
      "==================================================\n",
      "                                                 Name\n",
      "0                            22nd Century Group, Inc\n",
      "1              Aberdeen Asia-Pacific Income Fund Inc\n",
      "2                 Aberdeen Australia Equity Fund Inc\n",
      "3  Aberdeen Emerging Markets Equity Income Fund, ...\n",
      "4                  Aberdeen Global Income Fund, Inc.\n",
      "==================================================\n",
      "\n",
      "NASDAQ.XLSX (3447, 9) \n",
      "==================================================\n",
      "                                      Name\n",
      "0                               111, Inc.\n",
      "1  1347 Property Insurance Holdings, Inc.\n",
      "2  1347 Property Insurance Holdings, Inc.\n",
      "3                180 Degree Capital Corp.\n",
      "4                 1-800 FLOWERS.COM, Inc.\n",
      "==================================================\n",
      "\n",
      "NYSE.XLSX (3108, 9) \n",
      "==================================================\n",
      "                      Name\n",
      "0  3D Systems Corporation\n",
      "1              3M Company\n",
      "2         500.com Limited\n",
      "3             58.com Inc.\n",
      "4                 8x8 Inc\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stolis_df_list = []\n",
    "for f in files:\n",
    "    df = pd.read_csv( folder + f )\n",
    "    stolis_df_list.append(df)\n",
    "    print(f.upper(),df.shape, '\\n==================================================\\n',\n",
    "          df[['Name']].head() )\n",
    "    print('==================================================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 5304 companies, unique 4793 companies.\n"
     ]
    }
   ],
   "source": [
    "# concatenate companies from three exhanges\n",
    "stolis_df_ = pd.concat(stolis_df_list, axis = 0)\n",
    "\n",
    "# drop out fund\n",
    "stolis_df_ = stolis_df_[stolis_df_['industry'] == stolis_df_['industry']]\n",
    "\n",
    "# drop dupplicated company names\n",
    "stolis_df = stolis_df_.drop_duplicates(['Name']).reset_index(drop = True)\n",
    "\n",
    "print('Total %s companies, unique %s companies.' % (stolis_df_.shape[0], stolis_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare tweets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'Data handle.ipynb',\n",
       " 'data.xlsx',\n",
       " 'event_selecting_logic.md',\n",
       " 'marked_tweets.xlsx',\n",
       " 'nasdaq.xls',\n",
       " 'stock_list',\n",
       " 'tweets.txt',\n",
       " 'tweets_data.xlsx',\n",
       " 'words_database.xlsx',\n",
       " 'words_databse(1st_cleaned).xlsx',\n",
       " 'word_list.csv',\n",
       " 'yahoo_crawler.py']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename\n",
    "filename = 'tweets.txt'\n",
    "\n",
    "# read txt file\n",
    "file = open(filename).read()\n",
    "\n",
    "# convert json format to dataframe\n",
    "data = pd.DataFrame(eval(file.replace('false', 'False').replace('true', 'True')))\n",
    "\n",
    "# store tweets in excel\n",
    "data.to_excel('data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in(string, str_lst = ['data'], lower = True):\n",
    "    ''' Detect whether words in *str_lst* exist in *string* or not.\n",
    "    Input:\n",
    "    \n",
    "    -- string: string for examing\n",
    "               str format\n",
    "               \n",
    "    -- str_lst: a list of key words\n",
    "                list of str\n",
    "                default is *[' data ']*\n",
    "    \n",
    "    -- lower: determine whether capital letter is ignored or not, \n",
    "              True -> ignore capital letters, transform all string to lower case;\n",
    "              Fasle -> capital letters can't be ignored, both in *string* and *str_lst*.\n",
    "    \n",
    "    Output:\n",
    "    \n",
    "    -- if any key words is detedted:\n",
    "           return a string with all key words emphathized\n",
    "           \n",
    "       else:\n",
    "           return numpy.nan\n",
    "    '''\n",
    "    string = ' %s '%string\n",
    "    if lower:\n",
    "        \n",
    "        def my_lable(string, str_lst = str_lst):\n",
    "            for s in str_lst:\n",
    "                string = re.sub(r'(?<=\\W)%s(?=\\W)'%s.lower(), ' {%s} '%s.upper(), string )\n",
    "            return string\n",
    "\n",
    "        string = string.lower()\n",
    "\n",
    "        if my_lable(string, str_lst = str_lst) != string:\n",
    "            return my_lable(string)\n",
    "\n",
    "        else:\n",
    "            return np.nan\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        def my_lable(string, str_lst = str_lst):\n",
    "            for s in str_lst:\n",
    "                string = re.sub(r'(?<=\\W)%s(?=\\W)'%s, ' {%s} '%s.upper(), string )\n",
    "            return string\n",
    "\n",
    "        string = string\n",
    "\n",
    "        if my_lable(string, str_lst = str_lst) != string:\n",
    "            return my_lable(string)\n",
    "\n",
    "        else:\n",
    "            return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pattern(string, dict_):\n",
    "    pattern = re.compile(r'{ (.*?) }', re.S)\n",
    "    items = re.findall(pattern, string)\n",
    "    for i in items:\n",
    "        try:\n",
    "            dict_[i] += 1.\n",
    "        except:\n",
    "            dict_[i] = 1.\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['created_at', 'favorite_count', 'id_str', 'is_retweet', 'retweet_count',\n",
       "       'source', 'text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['created_at'] = pd.to_datetime( data['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: re.sub('https://\\S+', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search STR_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def search_str_lst(str_lst, print_ = True):\n",
    "    assert type(str_lst) == list, 'Type of str_lst must be list!'\n",
    "    i = 1\n",
    "    dict_ = {}\n",
    "    v_lst = data['text'].apply(is_in, str_lst= str_lst).dropna().values\n",
    "    if print_:\n",
    "        for v in v_lst:\n",
    "            dict_ = count_pattern(v, dict_)\n",
    "            print(v)\n",
    "            print('================================================%s/%s\\n'%( i, len(v_lst) ))\n",
    "            i += 1\n",
    "    return v_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " i  {AGREE}  with kim jong un of north korea that our personal relationship remains very good, perhaps the term excellent would be even more accurate, and that a third summit would be good in that we fully understand where we each stand. north korea has tremendous potential for....... \n",
      "================================================1/30\n",
      "\n",
      " more apprehensions (captures)\n",
      "at the southern border than in many years. border patrol amazing! country is full! system has been broken for many years. democrats in congress must  {AGREE}  to fix loopholes - no open borders (crimes &amp; drugs). will close southern border if necessary... \n",
      "================================================2/30\n",
      "\n",
      " “the lowest average jobs number for any president since 1951, 4.1%. economy doing great. if the democrats win, it is all over.” @varneyco  @foxandfriends  i  {AGREE} ! \n",
      "================================================3/30\n",
      "\n",
      " wow! a suffolk/usa today poll, just out, states, “50% of americans  {AGREE}  that  robert mueller’s investigation is a witch hunt.” @msnbc  very few think it is legit! we will soon find out? \n",
      "================================================4/30\n",
      "\n",
      " prominent legal scholars  {AGREE}  that our actions to address the national emergency at the southern border and to protect the american people are both constitutional and expressly authorized by congress.... \n",
      "================================================5/30\n",
      "\n",
      " i  {AGREE}  with rand paul. this is a total disgrace and should never happen to another president!  \n",
      "================================================6/30\n",
      "\n",
      " rt @gopchairwoman: i completely  {AGREE}  with kevin mccarthy.… \n",
      "================================================7/30\n",
      "\n",
      " ....this will be remembered as one of the most shocking votes in the history of congress. if there is one thing we should all  {AGREE}  on, it’s protecting the lives of innocent babies. \n",
      "================================================8/30\n",
      "\n",
      " rt @club4growth: we  {AGREE} ! the time is now! ?? \"the time has come to pass school choice for america's children.\" - @realdonaldtrump #sotu \n",
      "================================================9/30\n",
      "\n",
      " ....meeting with their top leaders and representatives today in the oval office. no final deal will be made until my friend president xi, and i, meet in the near future to discuss and  {AGREE}  on some of the long standing and more difficult points. very comprehensive transaction.... \n",
      "================================================10/30\n",
      "\n",
      " “in the media’s effort to destroy the president, they are actually destroying themselves. given all of the tremendous headwinds this president has faced, it’s amazing he has accomplished so much.” deroy murdock @foxandfriends  i  {AGREE} ! \n",
      "================================================11/30\n",
      "\n",
      " howard schultz doesn’t have the “guts” to run for president! watched him on @60minutes last night and i  {AGREE}  with him that he is not the “smartest person.” besides, america already has that! i only hope that starbucks is still paying me their rent in trump tower! \n",
      "================================================12/30\n",
      "\n",
      " president and mrs. obama built/has a ten foot wall around their d.c. mansion/compound. i  {AGREE} , totally necessary for their safety and security. the u.s. needs the same thing, slightly larger version! \n",
      "================================================13/30\n",
      "\n",
      " i totally  {AGREE} !  \n",
      "================================================14/30\n",
      "\n",
      " thanks @randpaul  “i am very proud of the president. this is exactly what he promised, and i think the people  {AGREE}  with him. we’ve been at war too long and in too many places...spent several trillion dollars on these wars everywhere. he’s different...that’s why he got elected.” \n",
      "================================================15/30\n",
      "\n",
      " “i’m proud of the president today to hear that he is declaring victory in syria.” senator rand paul.  “i couldn’t  {AGREE}  more with the presidents decision. by definition, this is the opposite of an obama decision. senator mike lee \n",
      "================================================16/30\n",
      "\n",
      " statement from china: “the teams of both sides are now having smooth communications and good cooperation with each other. we are full of confidence that an agreement can be reached within the next 90 days.” i  {AGREE} ! \n",
      "================================================17/30\n",
      "\n",
      " “it’s a mean &amp; nasty world out there, the middle east in particular. this is a long and historic commitment, &amp; one that is absolutely vital to america’s national security.” @secpompeo  i  {AGREE}  100%. in addition, many billions of dollars of purchases made in u.s., big jobs &amp; oil! \n",
      "================================================18/30\n",
      "\n",
      " .@davidasmanfox  “how do the democrats respond to this? think of how his position with republicans improves-all the candidates who won tonight. they realize how important he is because of what he did in campaigning for them. they owe him their political career.” thanks, i  {AGREE} ! \n",
      "================================================19/30\n",
      "\n",
      " so-called birthright citizenship, which costs our country billions of dollars and is very unfair to our citizens, will be ended one way or the other. it is not covered by the 14th amendment because of the words “subject to the jurisdiction thereof.” many legal scholars  {AGREE} ..... \n",
      "================================================20/30\n",
      "\n",
      " ...i  {AGREE}  with their stance 100%, and the united states is likewise taking a very hard line on illegal immigration. the prime minister is working very hard on the economy of italy - he will be successful! \n",
      "================================================21/30\n",
      "\n",
      " i  {AGREE}  wholeheartedly!  \n",
      "================================================22/30\n",
      "\n",
      " i  {AGREE}  with president obama 100%!  \n",
      "================================================23/30\n",
      "\n",
      " if the democrats would stop being obstructionists and come together, we could write up and  {AGREE}  to new immigration laws in less than one hour. look at the needless  pain and suffering that they are causing. look at the horrors taking place on the border. chuck &amp; nancy, call me! \n",
      "================================================24/30\n",
      "\n",
      " chuck schumer, i  {AGREE} !  \n",
      "================================================25/30\n",
      "\n",
      " the economy is doing perhaps better than ever before, and that’s prior to fixing some of the worst and most unfair trade deals ever made by any country. in any event, they are coming along very well. most countries  {AGREE}  that they must be changed, but nobody ever asked! \n",
      "================================================26/30\n",
      "\n",
      " democrats mistakenly tweet 2014 pictures from obama’s term showing children from the border in steel cages. they thought it was recent pictures in order to make us look bad, but backfires. dems must  {AGREE}  to wall and new border protection for good of country...bipartisan bill! \n",
      "================================================27/30\n",
      "\n",
      " department of justice should have urged the supreme court to at least hear the drivers license case on illegal immigrants in arizona. i  {AGREE}  with @loudobbs. should have sought review. \n",
      "================================================28/30\n",
      "\n",
      " “he’s got a very good point. somebody in the justice department has a treasure trove of evidence of mrs. clinton’s criminality at her own hands, or through others, that ought to be investigated. i fully  {AGREE}  with the president on that.” @judgenapolitano on @marthamaccallum show \n",
      "================================================29/30\n",
      "\n",
      " thank you @gopleader kevin mccarthy! couldn’t  {AGREE}  w/you more. together, we are #maga????  \n",
      "================================================30/30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "v_lst = search_str_lst(['AGREE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drop_lst = '''Company,Laboratories,Inc,plc,corp,group,equities,pharmaceutical,technology,plc,coporation,co,Resource,\n",
    "networks,green,texas,holdings,Inc,properties,holdings,energy,communications,limited,solutions,resources,brands,SUMMIT,\n",
    "hunt,companies,health,restrants,services,chemical,int,l,arts,resources,Holdings,Holding,Inc,Cos,Ltd,Corp,Co,plc,PEOPLE,\n",
    "red,space,under,Cos,Group,properties,Corporation,Incorporated,tree,business,city,Residential,Company,TOTAL,one,aid,up,\n",
    "line,gas,network,black,federal,union ,best,air,water,U,S,Trust,Arts,Communications,Chemical,Lifesciences,JUST,usa,\n",
    "Technologies,Systems,General,First,Street,Southern,Networks,Realty,Service,Class,A,Materials,Class,Cruise,Line,180,\n",
    "Services,Financial,Resources,NATIONAL ,Foods,Scientific,Beauty,Realty,Communications,com,Automotive,Stores,Mueller,\n",
    "Technologies,International,WEST ,Markets,Machines,Sciences,Exchange,Tool,Works,Dynamics,Bank,Investment,limited,Simply,\n",
    "Laboratories,NEWS ,technology,Resources,Resorts,equities,energy,health,Parts,brands,and,a,at,on,take,of,Church,forward,\n",
    "system,new,UNITED,Republic,OIL ,real,york,the,AMERICAN, america,state,C,Data,SECURITY ,companies,restrants,PLANS,can,\n",
    "Industries,Gold,Management,Education,REIT,Acquisition,Partners,LP,China,Hospitality,Medical,Capital,Royalty,world,\n",
    "Electronics,39,Enterprisesde,Investors,Industrial,Power,Products,Property,Insurance,Finance,Life,Worldwide,Electric,\n",
    "if,now,all,care,nation,by,do,mexico,it,in,me,CHECK,great,sports,good,golf,big,AGREE,GROWTH,north,world,korea,forward,\n",
    "Software,Pacific,Global,Bio,Entertainment,Media,Community,Estate,LLC,Hotels,for,Cool,first,second,third,fifth,fourth,\n",
    "Strong,Healthcare,Cohen,Standard,Star,Opportunity,Level,Plus,HealthCare,Georgia,Place,States,Information,800,Wisconsin,\n",
    "Source,Mark,Public,Way,Manufacturing,Funding,Better,South,Carolina,James,Beyond,Stock,Private,Career,European,Point,\n",
    "Children,Citizens,Clean,Center,Consumer,County,Old,Country,Credit,Journal,Dollar,Victory,TRADE,Montana,East,Focus,\n",
    "Ever,Live,Payments,Washington,Stay,Farmers,Choice,Indiana,Foundation,US,Five,Prime,Full,House,Fusion,Future,Times,Fix,\n",
    "Troy,Henry,Home,Hope,Building,Infrastructure,Support,Money,Japan,Control,John,Kelly,Defense,End,Smart,Marine,Merit,\n",
    "Con,Modern,Mr,My,NICE,Office,Ohio,Steel,Stop,Open,Re,Patrick,Virginia,Points,Popular,Positive,Progress,Safe,Safety,\n",
    "Special,Games,Florida,Number,SMART,Missouri,Spirit,Market,support,Price,Two,Long,Joint,Meet,Trade,Top,TOP,Tuesday,de,\n",
    "Wins,500,Interest,Communities,Pittsburgh,Lots,Cia,Family,Build,Canada,Cars,Fair,Israel,Clear,well,40,,Fire,Champion,\n",
    "Dr,Government,Mississippi,Far,Fortune,Host,Game,Las,Argentina,Europe,Party,Pennsylvania,Post,RE,Ready,Robert,San,Six,\n",
    "Joe,Team,France,Tennessee,AS,Met,Waters,White,Therapeutics,CALIFORNIA,strong,Recovery,Pharmaceuticals,Morning,Harvard,\n",
    "flags, seven, strategic, foot, ca, institute, origin, commercial, waste, pro,Flow, paper, achieve, display,invitation,\n",
    "art, island, grand, Chicago,Car, champions, associated, internet, super , blue, regional, purpose, Northeast ,Stone, \n",
    "age, box, midwest, yard,urban, ms, daily ,st, environment, independent,Events, zealand, inspired, ag, fixed, randny,\n",
    "participation, struggles, gain ,block, science,Saving, pt, fat, overseas, revolution, brand, agricultural, exact, \n",
    "advanced, path, construction,Suburban, rapid, location, buy, senior, distance, project, el , color, self, display, \n",
    "rocket, Direct, initiative, prosperity, club, research, physical, tower, lifetime, product, food,Development, stage, \n",
    "vacation, forum, test, tech, sound, coast, handling, universe, contract,Fund, priority, British, concrete, emissions,\n",
    "saving, four, natural, extended truck, light,Supply, forest, social, bar, magic, lake, pace, walker, achieve, pride, \n",
    "lines, dean, heritage, Music, digital, highway, shake, pan, ocean, nationwide, garden, grill, grid, customers, beach,\n",
    "Integrated, reading, development, wave, ocean, match, central, age, restaurant, Yale, mid,Front, platform, goods, \n",
    "cross, ms, produce, turning, site, budget\n",
    "'''.replace(' ', '').replace('\\n', '').lower().split(',')\n",
    "\n",
    "df = pd.read_excel('words_databse(1st_cleaned).xlsx')\n",
    "drop_lst += df[df['drop'] == 0].index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_set = {*drop_lst}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Company name search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22nd Century Group, Inc</td>\n",
       "      <td>[22nd, Century, Group, Inc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acme United Corporation.</td>\n",
       "      <td>[Acme, United, Corporation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Actinium Pharmaceuticals, Inc.</td>\n",
       "      <td>[Actinium, Pharmaceuticals, Inc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adams Resources &amp; Energy, Inc.</td>\n",
       "      <td>[Adams, Resources, Energy, Inc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AeroCentury Corp.</td>\n",
       "      <td>[AeroCentury, Corp]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Name                             words\n",
       "0         22nd Century Group, Inc       [22nd, Century, Group, Inc]\n",
       "1        Acme United Corporation.       [Acme, United, Corporation]\n",
       "2  Actinium Pharmaceuticals, Inc.  [Actinium, Pharmaceuticals, Inc]\n",
       "3  Adams Resources & Energy, Inc.   [Adams, Resources, Energy, Inc]\n",
       "4               AeroCentury Corp.               [AeroCentury, Corp]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split stock names\n",
    "name_df = stolis_df[['Name']].copy()\n",
    "\n",
    "name_df['words'] = pd.DataFrame( name_df['Name'].apply(lambda x: \\\n",
    "                                                        re.findall(r'(\\w+)', x) ) )\n",
    "name_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knock_out(lst, drop_lst = drop_set):\n",
    "    ''' Knock out words in drop_set\n",
    "    '''\n",
    "    lst_ = []\n",
    "    for i in lst:\n",
    "        # if in drop_lst or length < 2, drop it\n",
    "        if i.lower() in drop_lst or len(i)< 2:\n",
    "            pass\n",
    "        else:\n",
    "            lst_.append(i)\n",
    "            \n",
    "    if len(lst_) > 0:\n",
    "        return lst_\n",
    "    \n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "name_df['dropped'] = name_df['words'].apply(knock_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279 companies are dropped out from stock list.\n"
     ]
    }
   ],
   "source": [
    "dropped_companies = name_df[name_df['dropped'] != name_df['dropped']]\n",
    "print('%s companies are dropped out from stock list.'%dropped_companies.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4514 companies are kept after dropping out.\n"
     ]
    }
   ],
   "source": [
    "name_df = name_df.dropna()\n",
    "print('%s companies are kept after dropping out.'%name_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-arrange key-words and companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>words</th>\n",
       "      <th>dropped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22nd Century Group, Inc</td>\n",
       "      <td>[22nd, Century, Group, Inc]</td>\n",
       "      <td>[Century]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acme United Corporation.</td>\n",
       "      <td>[Acme, United, Corporation]</td>\n",
       "      <td>[Acme]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Actinium Pharmaceuticals, Inc.</td>\n",
       "      <td>[Actinium, Pharmaceuticals, Inc]</td>\n",
       "      <td>[Actinium]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adams Resources &amp; Energy, Inc.</td>\n",
       "      <td>[Adams, Resources, Energy, Inc]</td>\n",
       "      <td>[Adams]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AeroCentury Corp.</td>\n",
       "      <td>[AeroCentury, Corp]</td>\n",
       "      <td>[AeroCentury]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Name                             words  \\\n",
       "0         22nd Century Group, Inc       [22nd, Century, Group, Inc]   \n",
       "1        Acme United Corporation.       [Acme, United, Corporation]   \n",
       "2  Actinium Pharmaceuticals, Inc.  [Actinium, Pharmaceuticals, Inc]   \n",
       "3  Adams Resources & Energy, Inc.   [Adams, Resources, Energy, Inc]   \n",
       "4               AeroCentury Corp.               [AeroCentury, Corp]   \n",
       "\n",
       "         dropped  \n",
       "0      [Century]  \n",
       "1         [Acme]  \n",
       "2     [Actinium]  \n",
       "3        [Adams]  \n",
       "4  [AeroCentury]  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_words_dict_data():\n",
    "    words_dict = {}\n",
    "    for i in range( name_df.shape[0] ):\n",
    "        words = name_df['dropped'].values[i]\n",
    "        comp  = name_df['Name'   ].values[i]\n",
    "\n",
    "        for w in words:\n",
    "            try:\n",
    "                words_dict[w] += '%s, '%comp\n",
    "            except:\n",
    "                words_dict[w] = ''\n",
    "                words_dict[w] += '%s, '%comp\n",
    "    pd.DataFrame(words_dict, index = [0]).T.to_excel('words_database.xlsx')\n",
    "    pass\n",
    "save_words_dict_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_dict = {}\n",
    "for i in range( name_df.shape[0] ):\n",
    "    words = name_df['dropped'].values[i]\n",
    "    comp  = name_df['Name'   ].values[i]\n",
    "    \n",
    "    for w in words:\n",
    "        try:\n",
    "            words_dict[w] += [comp]\n",
    "        except:\n",
    "            words_dict[w] = []\n",
    "            words_dict[w] += [comp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop zero search frequency key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.2366 sec(s) elapsed, 411.6763 sec(s) left, total 420.9129 sec(s)\n",
      "18.0356 sec(s) elapsed, 392.9061 sec(s) left, total 410.9418 sec(s)\n",
      "27.1570 sec(s) elapsed, 385.3574 sec(s) left, total 412.5144 sec(s)\n",
      "35.9779 sec(s) elapsed, 373.9004 sec(s) left, total 409.8783 sec(s)\n",
      "44.8189 sec(s) elapsed, 363.6606 sec(s) left, total 408.4795 sec(s)\n",
      "54.7652 sec(s) elapsed, 361.1765 sec(s) left, total 415.9416 sec(s)\n",
      "65.9072 sec(s) elapsed, 363.1485 sec(s) left, total 429.0557 sec(s)\n",
      "76.1186 sec(s) elapsed, 357.4717 sec(s) left, total 433.5903 sec(s)\n",
      "86.5496 sec(s) elapsed, 351.6800 sec(s) left, total 438.2296 sec(s)\n",
      "97.4272 sec(s) elapsed, 346.5486 sec(s) left, total 443.9759 sec(s)\n",
      "109.7375 sec(s) elapsed, 344.8749 sec(s) left, total 454.6124 sec(s)\n",
      "119.7557 sec(s) elapsed, 335.0167 sec(s) left, total 454.7724 sec(s)\n",
      "128.8656 sec(s) elapsed, 322.8578 sec(s) left, total 451.7234 sec(s)\n",
      "138.1233 sec(s) elapsed, 311.4681 sec(s) left, total 449.5914 sec(s)\n",
      "147.2521 sec(s) elapsed, 300.0998 sec(s) left, total 447.3520 sec(s)\n",
      "156.8080 sec(s) elapsed, 289.8007 sec(s) left, total 446.6087 sec(s)\n",
      "166.8686 sec(s) elapsed, 280.4375 sec(s) left, total 447.3061 sec(s)\n",
      "176.1949 sec(s) elapsed, 269.8718 sec(s) left, total 446.0667 sec(s)\n",
      "185.6452 sec(s) elapsed, 259.6102 sec(s) left, total 445.2553 sec(s)\n",
      "195.5762 sec(s) elapsed, 250.0441 sec(s) left, total 445.6203 sec(s)\n",
      "207.6927 sec(s) elapsed, 243.0004 sec(s) left, total 450.6931 sec(s)\n",
      "218.3997 sec(s) elapsed, 233.9855 sec(s) left, total 452.3853 sec(s)\n",
      "228.4165 sec(s) elapsed, 224.1461 sec(s) left, total 452.5627 sec(s)\n",
      "238.3231 sec(s) elapsed, 214.1929 sec(s) left, total 452.5161 sec(s)\n",
      "248.4786 sec(s) elapsed, 204.4482 sec(s) left, total 452.9268 sec(s)\n",
      "258.5907 sec(s) elapsed, 194.6392 sec(s) left, total 453.2299 sec(s)\n",
      "269.0687 sec(s) elapsed, 185.0595 sec(s) left, total 454.1282 sec(s)\n",
      "278.7365 sec(s) elapsed, 174.9072 sec(s) left, total 453.6437 sec(s)\n",
      "288.2372 sec(s) elapsed, 164.6927 sec(s) left, total 452.9299 sec(s)\n",
      "297.3470 sec(s) elapsed, 154.3231 sec(s) left, total 451.6701 sec(s)\n",
      "306.5672 sec(s) elapsed, 144.0866 sec(s) left, total 450.6538 sec(s)\n",
      "316.8474 sec(s) elapsed, 134.3631 sec(s) left, total 451.2105 sec(s)\n",
      "327.4563 sec(s) elapsed, 124.7311 sec(s) left, total 452.1873 sec(s)\n",
      "340.1895 sec(s) elapsed, 115.7645 sec(s) left, total 455.9540 sec(s)\n",
      "352.1530 sec(s) elapsed, 106.3502 sec(s) left, total 458.5032 sec(s)\n",
      "362.5639 sec(s) elapsed, 96.3816 sec(s) left, total 458.9455 sec(s)\n",
      "373.3083 sec(s) elapsed, 86.4663 sec(s) left, total 459.7746 sec(s)\n",
      "383.2829 sec(s) elapsed, 76.3540 sec(s) left, total 459.6369 sec(s)\n",
      "394.0561 sec(s) elapsed, 66.3833 sec(s) left, total 460.4394 sec(s)\n",
      "408.0523 sec(s) elapsed, 56.8213 sec(s) left, total 464.8736 sec(s)\n",
      "418.5675 sec(s) elapsed, 46.6550 sec(s) left, total 465.2225 sec(s)\n",
      "428.9854 sec(s) elapsed, 36.4638 sec(s) left, total 465.4492 sec(s)\n",
      "442.6912 sec(s) elapsed, 26.4585 sec(s) left, total 469.1497 sec(s)\n",
      "457.8783 sec(s) elapsed, 16.3379 sec(s) left, total 474.2163 sec(s)\n",
      "469.0570 sec(s) elapsed, 5.9414 sec(s) left, total 474.9984 sec(s)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "t = time.time()\n",
    "k_lst = words_dict.copy().keys()\n",
    "for k in k_lst:\n",
    "    if len(search_str_lst([k], False)) == 0:\n",
    "        words_dict.pop(k)\n",
    "        \n",
    "    i += 1\n",
    "    T = time.time() - t\n",
    "    v = T/i\n",
    "    Tt = len(k_lst) * v\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print('%.4f sec(s) elapsed,'%T, '%.4f sec(s) left,'%(Tt - T), 'total %.4f sec(s)'%Tt )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "481"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find mentioned companies and mark key words out in tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "t = time.time()\n",
    "def find_comp(string, words_dict = words_dict.copy(), lower = True,):\n",
    "    ''' Find companies\n",
    "    '''\n",
    "    #====================================================\n",
    "    string = ' %s '%string.lower()\n",
    "        \n",
    "    def my_label(string, str_lst):\n",
    "        for s in str_lst:\n",
    "            string = re.sub(r'(?<=\\W)%s(?=\\W)'%s.lower(), ' {%s} '%s.upper(), string )\n",
    "        return string\n",
    "    \n",
    "    str_lst  = []\n",
    "    comp_lst = []\n",
    "    \n",
    "    for k in words_dict.keys():\n",
    "        \n",
    "        comp = words_dict[k]\n",
    "        \n",
    "        if lower:\n",
    "            k = k.lower()\n",
    "            \n",
    "            if  string != my_label(string, str_lst = [k] ):\n",
    "                str_lst  += [k]\n",
    "                \n",
    "                comp_lst += comp\n",
    "            \n",
    "        else:\n",
    "            if  string != my_label(string, str_lst = [k] ):\n",
    "                str_lst  += [k]\n",
    "                \n",
    "                comp_lst += comp\n",
    "                \n",
    "    global i\n",
    "    i += 1\n",
    "    T = time.time() - t\n",
    "    v = T/i\n",
    "    Tt = data['text'].shape[0] * v\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print('%.4f sec(s) elapsed,'%T, '%.4f sec(s) left,'%(Tt - T), 'total %.4f sec(s)'%Tt )\n",
    "    \n",
    "    if len(str_lst) > 0:\n",
    "        return my_label(string, str_lst = {*str_lst}), list( {*comp_lst} )\n",
    "    \n",
    "    else:\n",
    "        return np.nan, np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2208 sec(s) elapsed, 56.9743 sec(s) left, total 58.1951 sec(s)\n",
      "2.5402 sec(s) elapsed, 58.0061 sec(s) left, total 60.5464 sec(s)\n",
      "3.4214 sec(s) elapsed, 50.9444 sec(s) left, total 54.3658 sec(s)\n",
      "4.1999 sec(s) elapsed, 45.8525 sec(s) left, total 50.0524 sec(s)\n",
      "4.9210 sec(s) elapsed, 41.9956 sec(s) left, total 46.9165 sec(s)\n",
      "5.6218 sec(s) elapsed, 39.0433 sec(s) left, total 44.6651 sec(s)\n",
      "6.7167 sec(s) elapsed, 39.0238 sec(s) left, total 45.7405 sec(s)\n",
      "7.5848 sec(s) elapsed, 37.6112 sec(s) left, total 45.1960 sec(s)\n",
      "8.6314 sec(s) elapsed, 37.0864 sec(s) left, total 45.7178 sec(s)\n",
      "9.6079 sec(s) elapsed, 36.1930 sec(s) left, total 45.8010 sec(s)\n",
      "10.5765 sec(s) elapsed, 35.2581 sec(s) left, total 45.8346 sec(s)\n",
      "11.3109 sec(s) elapsed, 33.6215 sec(s) left, total 44.9324 sec(s)\n",
      "12.1479 sec(s) elapsed, 32.3974 sec(s) left, total 44.5453 sec(s)\n",
      "12.9604 sec(s) elapsed, 31.1697 sec(s) left, total 44.1300 sec(s)\n",
      "13.7843 sec(s) elapsed, 30.0223 sec(s) left, total 43.8066 sec(s)\n",
      "14.5656 sec(s) elapsed, 28.8307 sec(s) left, total 43.3963 sec(s)\n",
      "15.3078 sec(s) elapsed, 27.6170 sec(s) left, total 42.9248 sec(s)\n",
      "16.0265 sec(s) elapsed, 26.4171 sec(s) left, total 42.4436 sec(s)\n",
      "16.7367 sec(s) elapsed, 25.2548 sec(s) left, total 41.9915 sec(s)\n",
      "17.4829 sec(s) elapsed, 24.1876 sec(s) left, total 41.6706 sec(s)\n",
      "18.0923 sec(s) elapsed, 22.9772 sec(s) left, total 41.0695 sec(s)\n",
      "18.7642 sec(s) elapsed, 21.8944 sec(s) left, total 40.6586 sec(s)\n",
      "19.3267 sec(s) elapsed, 20.7300 sec(s) left, total 40.0566 sec(s)\n",
      "20.0924 sec(s) elapsed, 19.8162 sec(s) left, total 39.9086 sec(s)\n",
      "20.7351 sec(s) elapsed, 18.8026 sec(s) left, total 39.5377 sec(s)\n",
      "21.3914 sec(s) elapsed, 17.8289 sec(s) left, total 39.2203 sec(s)\n",
      "22.1570 sec(s) elapsed, 16.9624 sec(s) left, total 39.1194 sec(s)\n",
      "23.2733 sec(s) elapsed, 16.3495 sec(s) left, total 39.6228 sec(s)\n",
      "24.7232 sec(s) elapsed, 15.9166 sec(s) left, total 40.6398 sec(s)\n",
      "25.4829 sec(s) elapsed, 15.0094 sec(s) left, total 40.4924 sec(s)\n",
      "26.4620 sec(s) elapsed, 14.2298 sec(s) left, total 40.6918 sec(s)\n",
      "27.2729 sec(s) elapsed, 13.3552 sec(s) left, total 40.6282 sec(s)\n",
      "28.1563 sec(s) elapsed, 12.5168 sec(s) left, total 40.6731 sec(s)\n",
      "28.8971 sec(s) elapsed, 11.6183 sec(s) left, total 40.5154 sec(s)\n",
      "29.6232 sec(s) elapsed, 10.7236 sec(s) left, total 40.3468 sec(s)\n",
      "30.6374 sec(s) elapsed, 9.9316 sec(s) left, total 40.5691 sec(s)\n",
      "31.6731 sec(s) elapsed, 9.1338 sec(s) left, total 40.8070 sec(s)\n",
      "32.4869 sec(s) elapsed, 8.2671 sec(s) left, total 40.7539 sec(s)\n",
      "33.5132 sec(s) elapsed, 7.4502 sec(s) left, total 40.9635 sec(s)\n",
      "34.3514 sec(s) elapsed, 6.5869 sec(s) left, total 40.9383 sec(s)\n",
      "35.6083 sec(s) elapsed, 5.7929 sec(s) left, total 41.4012 sec(s)\n",
      "36.4086 sec(s) elapsed, 4.9152 sec(s) left, total 41.3237 sec(s)\n",
      "37.1632 sec(s) elapsed, 4.0361 sec(s) left, total 41.1993 sec(s)\n",
      "37.9179 sec(s) elapsed, 3.1627 sec(s) left, total 41.0806 sec(s)\n",
      "38.9669 sec(s) elapsed, 2.3120 sec(s) left, total 41.2789 sec(s)\n",
      "40.8109 sec(s) elapsed, 1.4816 sec(s) left, total 42.2925 sec(s)\n",
      "41.8993 sec(s) elapsed, 0.5973 sec(s) left, total 42.4965 sec(s)\n"
     ]
    }
   ],
   "source": [
    "res = data['text'].apply(lambda x: pd.Series( find_comp(x) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.columns = ['tweets', 'companies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([res, data['created_at']], axis = 1).dropna().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_excel('marked_tweets.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

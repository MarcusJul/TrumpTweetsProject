{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update local stock list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_stocklist_data():\n",
    "    ''' Functions for update stocklist data\n",
    "    Source: www.nasdaq.com\n",
    "    '''\n",
    "    # create stock_list data folder\n",
    "    folder = os.getcwd() + '\\\\stock_list\\\\'\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    # soure url\n",
    "    url = 'https://www.nasdaq.com/screening/companies-by-name.aspx?letter=0&exchange=%s&render=download'\n",
    "\n",
    "    # available exhanges\n",
    "    exchange = ['nasdaq', 'nyse', 'amex']\n",
    "\n",
    "    for exchg in exchange:\n",
    "        resp = requests.get(url%exchg)\n",
    "        with open(folder + '%s.xlsx'%exchg, 'wb') as output:\n",
    "            output.write(resp.content)\n",
    "    pass\n",
    "\n",
    "# update stock_list\n",
    "update_stocklist_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read local stock list and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amex.xlsx', 'nasdaq.xlsx', 'nyse.xlsx']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data folder\n",
    "folder = os.getcwd() + '\\\\stock_list\\\\'\n",
    "\n",
    "# file names\n",
    "files = os.listdir( folder )\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMEX.XLSX (309, 9) \n",
      "==================================================\n",
      "                                                 Name\n",
      "0                            22nd Century Group, Inc\n",
      "1              Aberdeen Asia-Pacific Income Fund Inc\n",
      "2                 Aberdeen Australia Equity Fund Inc\n",
      "3  Aberdeen Emerging Markets Equity Income Fund, ...\n",
      "4                  Aberdeen Global Income Fund, Inc.\n",
      "==================================================\n",
      "\n",
      "NASDAQ.XLSX (3447, 9) \n",
      "==================================================\n",
      "                                      Name\n",
      "0                               111, Inc.\n",
      "1  1347 Property Insurance Holdings, Inc.\n",
      "2  1347 Property Insurance Holdings, Inc.\n",
      "3                180 Degree Capital Corp.\n",
      "4                 1-800 FLOWERS.COM, Inc.\n",
      "==================================================\n",
      "\n",
      "NYSE.XLSX (3108, 9) \n",
      "==================================================\n",
      "                      Name\n",
      "0  3D Systems Corporation\n",
      "1              3M Company\n",
      "2         500.com Limited\n",
      "3             58.com Inc.\n",
      "4                 8x8 Inc\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stolis_df_list = []\n",
    "for f in files:\n",
    "    df = pd.read_csv( folder + f )\n",
    "    stolis_df_list.append(df)\n",
    "    print(f.upper(),df.shape, '\\n==================================================\\n',\n",
    "          df[['Name']].head() )\n",
    "    print('==================================================\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 5304 companies, unique 4793 companies.\n"
     ]
    }
   ],
   "source": [
    "# concatenate companies from three exhanges\n",
    "stolis_df_ = pd.concat(stolis_df_list, axis = 0)\n",
    "\n",
    "# drop out fund\n",
    "stolis_df_ = stolis_df_[stolis_df_['industry'] == stolis_df_['industry']]\n",
    "\n",
    "# drop dupplicated company names\n",
    "stolis_df = stolis_df_.drop_duplicates(['Name']).reset_index(drop = True)\n",
    "\n",
    "print('Total %s companies, unique %s companies.' % (stolis_df_.shape[0], stolis_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare tweets data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'Data handle.ipynb',\n",
       " 'data.csv',\n",
       " 'event_selecting_logic.md',\n",
       " 'stock_list',\n",
       " 'tweets.txt',\n",
       " 'tweets_data.csv',\n",
       " 'tweets_data.xlsx',\n",
       " 'word frequency.csv',\n",
       " 'word_list.csv',\n",
       " 'word_lst.csv']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename\n",
    "filename = 'tweets.txt'\n",
    "\n",
    "# read txt file\n",
    "file = open(filename).read()\n",
    "\n",
    "# convert json format to dataframe\n",
    "data = pd.DataFrame(eval(file.replace('false', 'False').replace('true', 'True')))\n",
    "\n",
    "# store tweets in excel\n",
    "data.to_excel('data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_in(string, str_lst = ['data'], lower = True):\n",
    "    ''' Detect whether words in *str_lst* exist in *string* or not.\n",
    "    Input:\n",
    "    \n",
    "    -- string: string for examing\n",
    "               str format\n",
    "               \n",
    "    -- str_lst: a list of key words\n",
    "                list of str\n",
    "                default is *[' data ']*\n",
    "    \n",
    "    -- lower: determine whether capital letter is ignored or not, \n",
    "              True -> ignore capital letters, transform all string to lower case;\n",
    "              Fasle -> capital letters can't be ignored, both in *string* and *str_lst*.\n",
    "    \n",
    "    Output:\n",
    "    \n",
    "    -- if any key words is detedted:\n",
    "           return a string with all key words emphathized\n",
    "           \n",
    "       else:\n",
    "           return numpy.nan\n",
    "    '''\n",
    "    string = ' %s '%string\n",
    "    if lower:\n",
    "        \n",
    "        def my_lable(string, str_lst = str_lst):\n",
    "            for s in str_lst:\n",
    "                string = re.sub(r'(?<=\\W)%s(?=\\W)'%s.lower(), ' {%s} '%s.upper(), string )\n",
    "            return string\n",
    "\n",
    "        string = string.lower()\n",
    "\n",
    "        if my_lable(string, str_lst = str_lst) != string:\n",
    "            return my_lable(string)\n",
    "\n",
    "        else:\n",
    "            return np.nan\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        def my_lable(string, str_lst = str_lst):\n",
    "            for s in str_lst:\n",
    "                string = re.sub(r'(?<=\\W)%s(?=\\W)'%s, ' {%s} '%s.upper(), string )\n",
    "            return string\n",
    "\n",
    "        string = string\n",
    "\n",
    "        if my_lable(string, str_lst = str_lst) != string:\n",
    "            return my_lable(string)\n",
    "\n",
    "        else:\n",
    "            return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pattern(string, dict_):\n",
    "    pattern = re.compile(r'{ (.*?) }', re.S)\n",
    "    items = re.findall(pattern, string)\n",
    "    for i in items:\n",
    "        try:\n",
    "            dict_[i] += 1.\n",
    "        except:\n",
    "            dict_[i] = 1.\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['created_at', 'favorite_count', 'id_str', 'is_retweet', 'retweet_count',\n",
       "       'source', 'text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['created_at'] = pd.to_datetime( data['created_at'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: re.sub('https://\\S+', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search STR_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def search_str_lst(str_lst, print_ = True):\n",
    "    assert type(str_lst) == list, 'Type of str_lst must be list!'\n",
    "    i = 1\n",
    "    dict_ = {}\n",
    "    v_lst = data['text'].apply(is_in, str_lst= str_lst).dropna().values\n",
    "    if print_:\n",
    "        for v in v_lst:\n",
    "            dict_ = count_pattern(v, dict_)\n",
    "            print(v)\n",
    "            print('================================================%s/%s\\n'%( i, len(v_lst) ))\n",
    "            i += 1\n",
    "    return v_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " time magazine called to say that i was probably going to be named “man (person) of the year,” like last year, but i would have to  {AGREE}  to an interview and a major photo shoot. i said probably is no good and took a pass. thanks anyway! \n",
      "================================================1/9\n",
      "\n",
      " i  {AGREE}  getting tax cuts approved  is important (we will also get healthcare), but perhaps no administration has done more in its first..... \n",
      "================================================2/9\n",
      "\n",
      " thank you @geraldorivera @foxandfriends.  {AGREE} !  \n",
      "================================================3/9\n",
      "\n",
      " a great and important day at the united nations.met with leaders of many nations who  {AGREE}  with much (or all) of what i stated in my speech! \n",
      "================================================4/9\n",
      "\n",
      " while all  {AGREE}  the u. s. president has the complete power to pardon, why think of that when only crime so far is leaks against us.fake news \n",
      "================================================5/9\n",
      "\n",
      " we finally  {AGREE}  on something rosie.  \n",
      "================================================6/9\n",
      "\n",
      " \"one of the most effective press conferences i've ever seen!\" says rush limbaugh. many  {AGREE} .yet fake media  calls it differently! dishonest \n",
      "================================================7/9\n",
      "\n",
      " interesting that certain middle-eastern countries  {AGREE}  with the ban. they know if certain people are allowed in it's death &amp; destruction! \n",
      "================================================8/9\n",
      "\n",
      " peaceful protests are a hallmark of our democracy. even if i don't always  {AGREE} , i recognize the rights of people to express their views. \n",
      "================================================9/9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "v_lst = search_str_lst(['AGREE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drop_lst = '''Company,Laboratories,Inc,plc,corp,group,equities,pharmaceutical,technology,plc,coporation,co,Resource,\n",
    "networks,green,texas,holdings,Inc,properties,holdings,energy,communications,limited,solutions,resources,brands,SUMMIT,\n",
    "hunt,companies,health,restrants,services,chemical,int,l,arts,resources,Holdings,Holding,Inc,Cos,Ltd,Corp,Co,plc,PEOPLE,\n",
    "red,space,under,Cos,Group,properties,Corporation,Incorporated,tree,business,city,Residential,Company,TOTAL,one,aid,up,\n",
    "line,gas,network,black,federal,union ,best,air,water,U,S,Trust,Arts,Communications,Chemical,Lifesciences,JUST,usa,\n",
    "Technologies,Systems,General,First,Street,Southern,Networks,Realty,Service,Class,A,Materials,Class,Cruise,Line,180,\n",
    "Services,Financial,Resources,NATIONAL ,Foods,Scientific,Beauty,Realty,Communications,com,Automotive,Stores,Mueller,\n",
    "Technologies,International,WEST ,Markets,Machines,Sciences,Exchange,Tool,Works,Dynamics,Bank,Investment,limited,Simply,\n",
    "Laboratories,NEWS ,technology,Resources,Resorts,equities,energy,health,Parts,brands,and,a,at,on,take,of,Church,forward,\n",
    "system,new,UNITED,Republic,OIL ,real,york,the,AMERICAN, america,state,C,Data,SECURITY ,companies,restrants,PLANS,can,\n",
    "Industries,Gold,Management,Education,REIT,Acquisition,Partners,LP,China,Hospitality,Medical,Capital,Royalty,world,\n",
    "Electronics,39,Enterprisesde,Investors,Industrial,Power,Products,Property,Insurance,Finance,Life,Worldwide,Electric,\n",
    "if,now,all,care,nation,by,do,mexico,it,in,me,CHECK,great,sports,good,golf,big,AGREE,GROWTH,north,world,korea,forward,\n",
    "Software,Pacific,Global,Bio,Entertainment,Media,Community,Estate,LLC,Hotels,for,Cool,first,second,third,fifth,fourth,\n",
    "Strong,Healthcare,Cohen,Standard,Star,Opportunity,Level,Plus,HealthCare,Georgia,Place,States,Information,800,Wisconsin,\n",
    "Source,Mark,Public,Way,Manufacturing,Funding,Better,South,Carolina,James,Beyond,Stock,Private,Career,European,Point,\n",
    "Children,Citizens,Clean,Center,Consumer,County,Old,Country,Credit,Journal,Dollar,Victory,TRADE,Montana,East,Focus,\n",
    "Ever,Live,Payments,Washington,Stay,Farmers,Choice,Indiana,Foundation,US,Five,Prime,Full,House,Fusion,Future,Times,Fix,\n",
    "Troy,Henry,Home,Hope,Building,Infrastructure,Support,Money,Japan,Control,John,Kelly,Defense,End,Smart,Marine,Merit,\n",
    "Con,Modern,Mr,My,NICE,Office,Ohio,Steel,Stop,Open,Re,Patrick,Virginia,Points,Popular,Positive,Progress,Safe,Safety,\n",
    "Special,Games,Florida,Number,SMART,Missouri,Spirit,Market,support,Price,Two,Long,Joint,Meet,Trade,Top,TOP,Tuesday,de,\n",
    "Wins,500,Interest,Communities,Pittsburgh,Lots,Cia,Family,Build,Canada,Cars,Fair,Israel,Clear,well,40,,Fire,Champion,\n",
    "Dr,Government,Mississippi,Far,Fortune,Host,Game,Las,Argentina,Europe,Party,Pennsylvania,Post,RE,Ready,Robert,San,Six,\n",
    "Joe,Team,France,Tennessee,AS,Met,Waters,White,Therapeutics,CALIFORNIA,strong,Recovery,Pharmaceuticals,Morning,Harvard\n",
    "'''.replace(' ', '').replace('\\n', '').lower().split(',')\n",
    "\n",
    "df = pd.read_excel('words_databse(1st_cleaned).xlsx')\n",
    "drop_lst += df[df['drop'] == 0].index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_set = {*drop_lst}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Company name search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22nd Century Group, Inc</td>\n",
       "      <td>[22nd, Century, Group, Inc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acme United Corporation.</td>\n",
       "      <td>[Acme, United, Corporation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Actinium Pharmaceuticals, Inc.</td>\n",
       "      <td>[Actinium, Pharmaceuticals, Inc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adams Resources &amp; Energy, Inc.</td>\n",
       "      <td>[Adams, Resources, Energy, Inc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AeroCentury Corp.</td>\n",
       "      <td>[AeroCentury, Corp]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Name                             words\n",
       "0         22nd Century Group, Inc       [22nd, Century, Group, Inc]\n",
       "1        Acme United Corporation.       [Acme, United, Corporation]\n",
       "2  Actinium Pharmaceuticals, Inc.  [Actinium, Pharmaceuticals, Inc]\n",
       "3  Adams Resources & Energy, Inc.   [Adams, Resources, Energy, Inc]\n",
       "4               AeroCentury Corp.               [AeroCentury, Corp]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split stock names\n",
    "name_df = stolis_df[['Name']].copy()\n",
    "\n",
    "name_df['words'] = pd.DataFrame( name_df['Name'].apply(lambda x: \\\n",
    "                                                        re.findall(r'(\\w+)', x) ) )\n",
    "name_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knock_out(lst, drop_lst = drop_set):\n",
    "    ''' Knock out words in drop_set\n",
    "    '''\n",
    "    lst_ = []\n",
    "    for i in lst:\n",
    "        # if in drop_lst or length < 2, drop it\n",
    "        if i.lower() in drop_lst or len(i)< 2:\n",
    "            pass\n",
    "        else:\n",
    "            lst_.append(i)\n",
    "            \n",
    "    if len(lst_) > 0:\n",
    "        return lst_\n",
    "    \n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "name_df['dropped'] = name_df['words'].apply(knock_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214 companies are dropped out from stock list.\n"
     ]
    }
   ],
   "source": [
    "dropped_companies = name_df[name_df['dropped'] != name_df['dropped']]\n",
    "print('%s companies are dropped out from stock list.'%dropped_companies.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4579 companies are kept after dropping out.\n"
     ]
    }
   ],
   "source": [
    "name_df = name_df.dropna()\n",
    "print('%s companies are kept after dropping out.'%name_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-arrange key-words and companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>words</th>\n",
       "      <th>dropped</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22nd Century Group, Inc</td>\n",
       "      <td>[22nd, Century, Group, Inc]</td>\n",
       "      <td>[Century]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Acme United Corporation.</td>\n",
       "      <td>[Acme, United, Corporation]</td>\n",
       "      <td>[Acme]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Actinium Pharmaceuticals, Inc.</td>\n",
       "      <td>[Actinium, Pharmaceuticals, Inc]</td>\n",
       "      <td>[Actinium]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Adams Resources &amp; Energy, Inc.</td>\n",
       "      <td>[Adams, Resources, Energy, Inc]</td>\n",
       "      <td>[Adams]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AeroCentury Corp.</td>\n",
       "      <td>[AeroCentury, Corp]</td>\n",
       "      <td>[AeroCentury]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Name                             words  \\\n",
       "0         22nd Century Group, Inc       [22nd, Century, Group, Inc]   \n",
       "1        Acme United Corporation.       [Acme, United, Corporation]   \n",
       "2  Actinium Pharmaceuticals, Inc.  [Actinium, Pharmaceuticals, Inc]   \n",
       "3  Adams Resources & Energy, Inc.   [Adams, Resources, Energy, Inc]   \n",
       "4               AeroCentury Corp.               [AeroCentury, Corp]   \n",
       "\n",
       "         dropped  \n",
       "0      [Century]  \n",
       "1         [Acme]  \n",
       "2     [Actinium]  \n",
       "3        [Adams]  \n",
       "4  [AeroCentury]  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_words_dict_data():\n",
    "    words_dict = {}\n",
    "    for i in range( name_df.shape[0] ):\n",
    "        words = name_df['dropped'].values[i]\n",
    "        comp  = name_df['Name'   ].values[i]\n",
    "\n",
    "        for w in words:\n",
    "            try:\n",
    "                words_dict[w] += '%s, '%comp\n",
    "            except:\n",
    "                words_dict[w] = ''\n",
    "                words_dict[w] += '%s, '%comp\n",
    "    pd.DataFrame(words_dict, index = [0]).T.to_excel('words_database.xlsx')\n",
    "    pass\n",
    "save_words_dict_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_dict = {}\n",
    "for i in range( name_df.shape[0] ):\n",
    "    words = name_df['dropped'].values[i]\n",
    "    comp  = name_df['Name'   ].values[i]\n",
    "    \n",
    "    for w in words:\n",
    "        try:\n",
    "            words_dict[w] += ['%s, '%comp]\n",
    "        except:\n",
    "            words_dict[w] = []\n",
    "            words_dict[w] += ['%s, '%comp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop zero search frequency key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1599 sec(s) elapsed, 191.1893 sec(s) left, total 195.3492 sec(s)\n",
      "7.8869 sec(s) elapsed, 177.2983 sec(s) left, total 185.1852 sec(s)\n",
      "11.7234 sec(s) elapsed, 171.7872 sec(s) left, total 183.5106 sec(s)\n",
      "15.9262 sec(s) elapsed, 171.0472 sec(s) left, total 186.9733 sec(s)\n",
      "19.6449 sec(s) elapsed, 164.8602 sec(s) left, total 184.5051 sec(s)\n",
      "23.5924 sec(s) elapsed, 161.0572 sec(s) left, total 184.6496 sec(s)\n",
      "27.5600 sec(s) elapsed, 157.3282 sec(s) left, total 184.8882 sec(s)\n",
      "31.3700 sec(s) elapsed, 152.7717 sec(s) left, total 184.1416 sec(s)\n",
      "35.1512 sec(s) elapsed, 148.2600 sec(s) left, total 183.4112 sec(s)\n",
      "38.7487 sec(s) elapsed, 143.2151 sec(s) left, total 181.9638 sec(s)\n",
      "42.5058 sec(s) elapsed, 138.9552 sec(s) left, total 181.4610 sec(s)\n",
      "46.8651 sec(s) elapsed, 136.5338 sec(s) left, total 183.3989 sec(s)\n",
      "50.9978 sec(s) elapsed, 133.2219 sec(s) left, total 184.2196 sec(s)\n",
      "54.9952 sec(s) elapsed, 129.4743 sec(s) left, total 184.4695 sec(s)\n",
      "58.6670 sec(s) elapsed, 124.9999 sec(s) left, total 183.6669 sec(s)\n",
      "62.2938 sec(s) elapsed, 120.5385 sec(s) left, total 182.8324 sec(s)\n",
      "65.9267 sec(s) elapsed, 116.1861 sec(s) left, total 182.1128 sec(s)\n",
      "69.6099 sec(s) elapsed, 111.9947 sec(s) left, total 181.6046 sec(s)\n",
      "73.2281 sec(s) elapsed, 107.7609 sec(s) left, total 180.9890 sec(s)\n",
      "76.8454 sec(s) elapsed, 103.5876 sec(s) left, total 180.4331 sec(s)\n",
      "80.4392 sec(s) elapsed, 99.4381 sec(s) left, total 179.8773 sec(s)\n",
      "84.0842 sec(s) elapsed, 95.3974 sec(s) left, total 179.4816 sec(s)\n",
      "88.0676 sec(s) elapsed, 91.7435 sec(s) left, total 179.8111 sec(s)\n",
      "91.8785 sec(s) elapsed, 87.8971 sec(s) left, total 179.7755 sec(s)\n",
      "95.5347 sec(s) elapsed, 83.9177 sec(s) left, total 179.4524 sec(s)\n",
      "99.1875 sec(s) elapsed, 79.9604 sec(s) left, total 179.1479 sec(s)\n",
      "102.8282 sec(s) elapsed, 76.0167 sec(s) left, total 178.8448 sec(s)\n",
      "106.4499 sec(s) elapsed, 72.0818 sec(s) left, total 178.5317 sec(s)\n",
      "110.0905 sec(s) elapsed, 68.1802 sec(s) left, total 178.2708 sec(s)\n",
      "113.7033 sec(s) elapsed, 64.2802 sec(s) left, total 177.9835 sec(s)\n",
      "117.3513 sec(s) elapsed, 60.4170 sec(s) left, total 177.7684 sec(s)\n",
      "120.9669 sec(s) elapsed, 56.5520 sec(s) left, total 177.5189 sec(s)\n",
      "124.6075 sec(s) elapsed, 52.7128 sec(s) left, total 177.3203 sec(s)\n",
      "128.2816 sec(s) elapsed, 48.8979 sec(s) left, total 177.1796 sec(s)\n",
      "131.9040 sec(s) elapsed, 45.0735 sec(s) left, total 176.9775 sec(s)\n",
      "135.5759 sec(s) elapsed, 41.2753 sec(s) left, total 176.8512 sec(s)\n",
      "139.1922 sec(s) elapsed, 37.4690 sec(s) left, total 176.6612 sec(s)\n",
      "142.7728 sec(s) elapsed, 33.6643 sec(s) left, total 176.4371 sec(s)\n",
      "146.4388 sec(s) elapsed, 29.8885 sec(s) left, total 176.3274 sec(s)\n",
      "150.1888 sec(s) elapsed, 26.1329 sec(s) left, total 176.3217 sec(s)\n",
      "154.4226 sec(s) elapsed, 22.4478 sec(s) left, total 176.8703 sec(s)\n",
      "158.7511 sec(s) elapsed, 18.7477 sec(s) left, total 177.4988 sec(s)\n",
      "162.7942 sec(s) elapsed, 14.9922 sec(s) left, total 177.7864 sec(s)\n",
      "166.4504 sec(s) elapsed, 11.1976 sec(s) left, total 177.6480 sec(s)\n",
      "170.0759 sec(s) elapsed, 7.4078 sec(s) left, total 177.4837 sec(s)\n",
      "174.0743 sec(s) elapsed, 3.6329 sec(s) left, total 177.7072 sec(s)\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "t = time.time()\n",
    "k_lst = words_dict.copy().keys()\n",
    "for k in k_lst:\n",
    "    if len(search_str_lst([k], False)) == 0:\n",
    "        words_dict.pop(k)\n",
    "        \n",
    "    i += 1\n",
    "    T = time.time() - t\n",
    "    v = T/i\n",
    "    Tt = len(k_lst) * v\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print('%.4f sec(s) elapsed,'%T, '%.4f sec(s) left,'%(Tt - T), 'total %.4f sec(s)'%Tt )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "426"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find mentioned companies and mark key words out in tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "t = time.time()\n",
    "def find_comp(string, words_dict = words_dict.copy(), lower = True,):\n",
    "    ''' Find companies\n",
    "    '''\n",
    "    #====================================================\n",
    "    string = ' %s '%string.lower()\n",
    "        \n",
    "    def my_label(string, str_lst):\n",
    "        for s in str_lst:\n",
    "            string = re.sub(r'(?<=\\W)%s(?=\\W)'%s.lower(), ' {%s} '%s.upper(), string )\n",
    "        return string\n",
    "    \n",
    "    str_lst  = []\n",
    "    comp_lst = []\n",
    "    \n",
    "    for k in words_dict.keys():\n",
    "        \n",
    "        comp = words_dict[k]\n",
    "        \n",
    "        if lower:\n",
    "            k = k.lower()\n",
    "            \n",
    "            if  string != my_label(string, str_lst = [k] ):\n",
    "                str_lst  += [k]\n",
    "                \n",
    "                comp_lst += comp\n",
    "            \n",
    "        else:\n",
    "            if  string != my_label(string, str_lst = [k] ):\n",
    "                str_lst  += [k]\n",
    "                \n",
    "                comp_lst += comp\n",
    "                \n",
    "    global i\n",
    "    i += 1\n",
    "    T = time.time() - t\n",
    "    v = T/i\n",
    "    Tt = data['text'].shape[0] * v\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        print('%.4f sec(s) elapsed,'%T, '%.4f sec(s) left,'%(Tt - T), 'total %.4f sec(s)'%Tt )\n",
    "    \n",
    "    if len(str_lst) > 0:\n",
    "        return my_label(string, str_lst = {*str_lst}), ''.join( list( {*comp_lst} ) )\n",
    "    \n",
    "    else:\n",
    "        return np.nan, np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7344 sec(s) elapsed, 18.3522 sec(s) left, total 19.0866 sec(s)\n",
      "1.3594 sec(s) elapsed, 16.3057 sec(s) left, total 17.6651 sec(s)\n",
      "1.9531 sec(s) elapsed, 14.9674 sec(s) left, total 16.9206 sec(s)\n",
      "2.4688 sec(s) elapsed, 13.5720 sec(s) left, total 16.0408 sec(s)\n",
      "2.9531 sec(s) elapsed, 12.3972 sec(s) left, total 15.3504 sec(s)\n",
      "3.4063 sec(s) elapsed, 11.3485 sec(s) left, total 14.7548 sec(s)\n",
      "3.8750 sec(s) elapsed, 10.5123 sec(s) left, total 14.3873 sec(s)\n",
      "4.3750 sec(s) elapsed, 9.8383 sec(s) left, total 14.2133 sec(s)\n",
      "4.8594 sec(s) elapsed, 9.1734 sec(s) left, total 14.0328 sec(s)\n",
      "5.3281 sec(s) elapsed, 8.5197 sec(s) left, total 13.8478 sec(s)\n",
      "5.8281 sec(s) elapsed, 7.9421 sec(s) left, total 13.7703 sec(s)\n",
      "6.2969 sec(s) elapsed, 7.3411 sec(s) left, total 13.6380 sec(s)\n",
      "6.8125 sec(s) elapsed, 6.8073 sec(s) left, total 13.6198 sec(s)\n",
      "7.3058 sec(s) elapsed, 6.2569 sec(s) left, total 13.5627 sec(s)\n",
      "7.7745 sec(s) elapsed, 5.6961 sec(s) left, total 13.4707 sec(s)\n",
      "8.2471 sec(s) elapsed, 5.1493 sec(s) left, total 13.3964 sec(s)\n",
      "8.7628 sec(s) elapsed, 4.6340 sec(s) left, total 13.3967 sec(s)\n",
      "9.3215 sec(s) elapsed, 4.1377 sec(s) left, total 13.4593 sec(s)\n",
      "9.7903 sec(s) elapsed, 3.6018 sec(s) left, total 13.3921 sec(s)\n",
      "10.3938 sec(s) elapsed, 3.1130 sec(s) left, total 13.5068 sec(s)\n",
      "10.9876 sec(s) elapsed, 2.6109 sec(s) left, total 13.5985 sec(s)\n",
      "11.4789 sec(s) elapsed, 2.0819 sec(s) left, total 13.5607 sec(s)\n",
      "11.9633 sec(s) elapsed, 1.5552 sec(s) left, total 13.5185 sec(s)\n",
      "12.6352 sec(s) elapsed, 1.0477 sec(s) left, total 13.6828 sec(s)\n",
      "13.2152 sec(s) elapsed, 0.5233 sec(s) left, total 13.7386 sec(s)\n"
     ]
    }
   ],
   "source": [
    "res = data['text'].apply(lambda x: pd.Series( find_comp(x) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.columns = ['tweets', 'companies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.concat([res, data['created_at']], axis = 1).dropna().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_excel('marked_tweets.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
